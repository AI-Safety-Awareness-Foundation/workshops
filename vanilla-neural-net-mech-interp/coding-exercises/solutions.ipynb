{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c49c4960",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "This notebook is meant to be an interactive exploration of how we can use a\n",
    "key-value perspective on a single hidden-layer neural net to get much more\n",
    "interpretable results about how a basic neural net is able to do image\n",
    "recognition of hand-written digits from the MNIST dataset.\n",
    "\n",
    "This is meant to be used alongside a set of slides found in the associated\n",
    "GitHub repository.\n",
    "\n",
    "By the end of this set of exercises you will have acccomplished the following:\n",
    "\n",
    "1. Analyzed why a vanilla neural net was able to correctly identify certain\n",
    "handwritten digits\n",
    "2. Analyzed why a vanilla neural net got some handwritten digits wrong\n",
    "3. Used this understanding to specifically knock out the neural nets ability to\n",
    "recognize one specific digit\n",
    "\n",
    "Taken together, this means you will have gained a level of insight into a\n",
    "vanilla neural net which makes the net significantly less \"black box-y\" than is\n",
    "usually claimed. Along the way you'll get a bit of a taste of how mechanistic\n",
    "interpretability works, albeit for a much more simplified model than what we\n",
    "encounter in cutting-edge AI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f390fdfc",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "__If you are running this in Google Colab make sure to uncomment the following\n",
    "lines of code that are prefixed by exclamation marks.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d70f3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Not provided by default in Google Colab\n",
    "!pip install jaxtyping\n",
    "\n",
    "# The rest downloads some binary files we'll need\n",
    "!pip install gdown\n",
    "\n",
    "# Download all the pretrained models\n",
    "\n",
    "!gdown --id 1F2Z8ziHPaXd_GT_fYe974ySiQVhW0yz0\n",
    "!gdown --id 1gGE9MtYvQwCevY9qx-3BEYyoIhDOOBBh\n",
    "!gdown --id 1mLEVHleRHiGLHKvu06LE0Oq2Mm6b2lCg\n",
    "!gdown --id 1KTZ9m4qmEmUIr-FZMSeVUkd4H8LSB_4b\n",
    "!gdown --id 1-Q16KxTkfg5hktOAE-OPgWglgGaaaHFd\n",
    "!gdown --id 1RFwpFgpPIOONABfXiHGsMtJdUF1gd10S\n",
    "!gdown --id 15BUINas3RQcUVml3QQvoOTVkZjd7IPFS\n",
    "!gdown --id 1ASEwxJHndnjFiu2G9tdmn3B5OJkvRQb7\n",
    "!gdown --id 1AggdX5mQ9o1QAy9P8_ZpckC8qRDTwPm1\n",
    "!gdown --id 1nMjFaMMtijoLidaJSMQoYGnQld5TKp7Q\n",
    "!gdown --id 19H7Y50yQsUBj9dWzHWOU8VJJ6zU4cQhA\n",
    "!gdown --id 1A5O3OatMM0Lj2m1655Nx1NSwoervV0mS\n",
    "!gdown --id 1rKFm7BY8eqQitHYDTrKHzWoFDE_e7fp0\n",
    "!gdown --id 1vNM6pD5gc4oOzumGtURp6ASw7nqtwinz\n",
    "!gdown --id 1_1GFcsjIuWRUcgPU9rMEY5g66K7eX93M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37589e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Some basic imports that will be necessary for any of our code to be able to\n",
    "# run.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "import einops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d751b2e5",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "Let's begin with a very quick look at the single hidden layer neural net we're\n",
    "using for our exercise today. This kind of basic neural net goes by many names,\n",
    "whether it's an MLP (Multi-Layer Perceptron), feed-forward neural net, vanilla\n",
    "neural net, etc.\n",
    "\n",
    "A neural net with a single hidden layer is enough to approximate any function if\n",
    "the net is large enough, so in theory could be used for many different tasks.\n",
    "\n",
    "In practice, for many domains it turns out it is extremely difficult to actually\n",
    "get a single hidden layer neural net to learn the relevant task at hand within a\n",
    "feasible computational and data budget. However, it turns out to be enough to\n",
    "get reasonable scores for recognizing hand-written digits in the MNIST dataset.\n",
    "\n",
    "Moreover, it's one of the most basic components of modern machine learning\n",
    "and AI that still exhibits enough complexity to be interesting and is often\n",
    "treated as an opaque black box we do not understand.\n",
    "\n",
    "This exploration will hopefully show you some ways of breaking open this black\n",
    "box and making it more understandable!\n",
    "\n",
    "For MNIST digit recognition, we'll set the input dimension to 784 (for 28x28\n",
    "images) and the output dimension to 10 (one for each category of digit). But\n",
    "we'll leave these flexible, as well as whether to use softmax or not, because\n",
    "we'll use this same architecture to illustrate some of the tiny neural nets we\n",
    "presented in the slides.\n",
    "\n",
    "Spend a few minutes reviewing the architecture of `SimpleNN` to make sure you\n",
    "understand what's going on. The default settings for `__init__` are all meant\n",
    "for our main MNIST digit recognition task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed956631",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "# simple NN with 3 layers\n",
    "class SimpleNN(nn.Module):\n",
    "  def __init__(self, hidden_dim: int, input_dim=784, output_dim=10, has_bias: bool=True, use_softmax: bool=True):\n",
    "    super(SimpleNN, self).__init__()\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.flatten = nn.Flatten()\n",
    "    self.fc1 = nn.Linear(input_dim, hidden_dim, bias=has_bias)\n",
    "    self.fc2 = nn.Linear(hidden_dim, output_dim, bias=has_bias)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.use_softmax = use_softmax\n",
    "    self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    assert len(x.shape) >= 3, f\"We expected a tensor with at least 3 dimensions, not {len(x.shape)} dimensions (the overall shape was {x.shape}). The reason we expect 3 dimensions (and not say just 2 dimensions for a single image with two dimensions), is that the neural net expects entire batches of input, e.g. if you have three 28x28 images, you should stack them together to make a 3x28x28 tensor. If you have only one 28x28 image, you should use unsqueeze to make it a 1x28x28 batch of images\"\n",
    "    x = self.flatten(x)\n",
    "    x1 = self.relu(self.fc1(x))\n",
    "    x2 = self.fc2(x1)\n",
    "    if self.use_softmax:\n",
    "      return self.softmax(x2)\n",
    "    else:\n",
    "      return x2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c03e8df",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Okay now we want to reframe our model using the key-value visualization So it's\n",
    "time to implement the functions that will pull out the ith key and ith value in\n",
    "our neural net.\n",
    "\n",
    "These functions are just one line each, but they are extremely important to\n",
    "understand. As a reminder, the connections between two layers of a neural net\n",
    "can be described as a matrix.\n",
    "\n",
    "For example, in the following neural net where A and B, C and D, and E and F all\n",
    "form respective neural net layers, this can be represented as two matrices\n",
    "describing the connections from the AB layer to the CD layer and then from the\n",
    "CD layer to the EF layer.\n",
    "\n",
    "```\n",
    "A -- C -- E\n",
    "  \\ /  \\ /\n",
    "  / \\  / \\ \n",
    "B -- D -- F\n",
    "```\n",
    "\n",
    "These two matrices look like\n",
    "\n",
    "$\\begin{bmatrix} AC & BC \\\\ AD & BD \\end{bmatrix}$\n",
    "\n",
    "and\n",
    "\n",
    "$\\begin{bmatrix} CE & DE \\\\ CF & DF \\end{bmatrix}$\n",
    "\n",
    "where e.g. $AC$ refers to the weight of the connection from $A$ to $C$, $BD$ to\n",
    "the weight of the connection from $B$ to $D$, etc.\n",
    "\n",
    "If we split the neural net into key-value pairs, we end up with the following:\n",
    "\n",
    "```\n",
    "A -- C -- E\n",
    "    /  \\  \n",
    "  /      \\ \n",
    "B         F\n",
    "```\n",
    "\n",
    "and \n",
    "\n",
    "```\n",
    "A         E\n",
    "  \\      /\n",
    "    \\  /   \n",
    "B -- D -- F\n",
    "```\n",
    ".\n",
    "\n",
    "If you observe the key and value of the first key value pair, you'll find that\n",
    "they are $\\begin{bmatrix} AC \\\\ BC \\end{bmatrix}$ and $\\begin{bmatrix} CE \\\\ CF\n",
    "\\end{bmatrix}$ respectively. That is the first key corresponds to the first row\n",
    "of the first matrix and the first value corresponds to the first column of the\n",
    "second matrix.\n",
    "\n",
    "This holds for the second key-value pair as well. More generally, the $i$-th\n",
    "key-value pair will have its key correspond to the $i$-th row of the first matrix\n",
    "and the $i$-th column of the second matrix.\n",
    "\n",
    "The reason we alternate between rows and columns of the respective matrices is\n",
    "that the rows of a matrix correspond to the weights of connections coming into a\n",
    "neuron and the columns of a matrix correspond to the weights of all the\n",
    "connections broadcasting out to the next set of neurons.\n",
    "\n",
    "The traditional view of neural nets where each neuron takes $n$ inputs and gives\n",
    "one output focuses primarily on this \"per-row\" perspective, but in our key-value\n",
    "perspective we combine the \"per-row\" and \"per-column\" perspective.\n",
    "\n",
    "This per-row and per-column perspective immediately leads to the one-line\n",
    "implementations of `pull_out_ith_key` and `pull_out_ith_value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7ab0fb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def pull_out_ith_key(model, i):\n",
    "  return model.fc1.weight[i]\n",
    "\n",
    "def pull_out_ith_value(model, i):\n",
    "  return model.fc2.weight[:, i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db17c97",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "This is all a lot of visualization code which you can either read or just run.\n",
    "\n",
    "Unlike the previous section, you don't need to understand these too well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387d12f9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "#plots the image\n",
    "def visualize_image(image):\n",
    "  norm = TwoSlopeNorm(vmin=-1, vcenter=0, vmax=1)\n",
    "  plt.imshow(image.detach().numpy(), cmap='seismic', norm=norm)\n",
    "  plt.axis('off')\n",
    "  plt.show()\n",
    "\n",
    "#plots a heatmap of a key\n",
    "def visualize_ith_key(model, i, x_size=28, y_size=28):\n",
    "  key = pull_out_ith_key(model, i).reshape(x_size, y_size)\n",
    "  if model.fc1.bias is not None:\n",
    "    key_bias = model.fc1.bias[i]\n",
    "  else:\n",
    "     key_bias = 0\n",
    "  norm = TwoSlopeNorm(vmin=-1, vcenter=0, vmax=1)\n",
    "  plt.imshow(key.detach().numpy(), cmap='seismic', norm=norm)\n",
    "  plt.axis('off')\n",
    "  plt.title(f'Key {i} (bias: {key_bias})')\n",
    "  plt.show()\n",
    "\n",
    "#visualizes a value\n",
    "def visualize_ith_value(model, i):\n",
    "  value = pull_out_ith_value(model, i).unsqueeze(0)\n",
    "  norm = TwoSlopeNorm(vmin=-1, vcenter=0, vmax=1)\n",
    "  plt.imshow(value.detach().numpy(), cmap='seismic', norm=norm)\n",
    "  for x in range(value.shape[1]):\n",
    "    plt.text(x, 0, f'{value[0, x].item():.3f}', ha='center', va='center', color='black', fontsize=8)\n",
    "  plt.axis('off')\n",
    "  plt.title(f'Value {i}')\n",
    "  plt.show()\n",
    "\n",
    "#visualizes the global value bias for each digit, or the baseline before any interactions\n",
    "def visualize_value_bias(model):\n",
    "  value = model.fc2.bias.unsqueeze(0)\n",
    "  norm = TwoSlopeNorm(vmin=-1, vcenter=0, vmax=1)\n",
    "  plt.imshow(value.detach().numpy(), cmap='seismic', norm=norm)\n",
    "  for x in range(value.shape[1]):\n",
    "    plt.text(x, 0, f'{value[0, x].item():.3f}', ha='center', va='center', color='black', fontsize=8)\n",
    "  plt.axis('off')\n",
    "  plt.title(f'Global value bias')\n",
    "  plt.show()\n",
    "\n",
    "#combines the above 3 visualization functions\n",
    "def visualize_ith_key_value(model, i, key_x_size=28, key_y_size=28):\n",
    "  visualize_ith_key(model, i, x_size=key_x_size, y_size=key_y_size)\n",
    "  visualize_ith_value(model, i)\n",
    "  if model.fc2.bias is not None:\n",
    "    visualize_value_bias(model)\n",
    "\n",
    "#Shows most influential interaction areas between an image and key \n",
    "def visualize_element_wise_multi_of_key_image(model, i, image, key_x_size=28, key_y_size=28):\n",
    "  key = model.fc1.weight[i].reshape(key_x_size, key_y_size)\n",
    "  element_wise_multi = key * image\n",
    "  norm = TwoSlopeNorm(vmin=-1, vcenter=0, vmax=1)\n",
    "  plt.imshow(element_wise_multi.detach().numpy(), cmap='seismic', norm=norm)\n",
    "  plt.axis('off')\n",
    "  plt.title(f'Element-wise multiplication of key {i} with image')\n",
    "  plt.show()\n",
    "  print(f\"Dot-Product: {torch.sum(element_wise_multi)}\")\n",
    "\n",
    "#combines all of the above visualization functions\n",
    "def visualize_ith_key_value_on_image(model, i, image, key_x_size=28, key_y_size=28):\n",
    "  visualize_ith_key_value(model, i, key_x_size=key_x_size, key_y_size=key_y_size)\n",
    "  visualize_element_wise_multi_of_key_image(model, i, image, key_x_size=key_x_size, key_y_size=key_y_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ff385a",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Let's play around a little with our visualization functions.\n",
    "\n",
    "We can being by visualizing a few 2x2 images.\n",
    "\n",
    "Instead of using black and white, as in our slides, we'll use two contrasting\n",
    "colors in addition to white, since once we plot things that aren't just image\n",
    "pixels, we may need to plot both positive and negative numbers. We'll use\n",
    "increasingly darker shades of red to indicate positive numbers of increasing\n",
    "magnitude, increasingly darker shades of blue to indicate negatives numbers of\n",
    "increasing magnitude, and white to indicate 0.\n",
    "\n",
    "This means most of our images of handwritten digits will consist of red and\n",
    "white (red for 1 and white for 0), although our keys and value vectors will\n",
    "consist of red, white, and blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f2d3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_0 = torch.Tensor(\n",
    "   [\n",
    "      [1, 0],\n",
    "      [1, 0],\n",
    "   ]\n",
    ")\n",
    "\n",
    "visualize_image(input_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fc074a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_1 = torch.Tensor(\n",
    "   [\n",
    "      [1, 0],\n",
    "      [1, 1],\n",
    "   ]\n",
    ")\n",
    "\n",
    "visualize_image(input_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2bd2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_2 = torch.Tensor(\n",
    "   [\n",
    "      [0, 0],\n",
    "      [1, 0],\n",
    "   ]\n",
    ")\n",
    "\n",
    "visualize_image(input_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7218222f",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "We now recreate the basic neural net we used in our slides to demonstrate the\n",
    "key-value decomposition of a neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a84365",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# When we first initialize a SimpleNN, all parameters \n",
    "example_nn = SimpleNN(hidden_dim=2, input_dim=4, output_dim=2, has_bias=False, use_softmax=False)\n",
    "preset_fc1 = nn.Parameter(\n",
    "  torch.Tensor(\n",
    "    [\n",
    "      [1, 0, 0, 0],\n",
    "      [0, 0, 0, 1],\n",
    "    ]\n",
    "  )\n",
    ")\n",
    "example_nn.fc1.weight = preset_fc1\n",
    "\n",
    "preset_fc2 = nn.Parameter(torch.Tensor(\n",
    "   [\n",
    "      [1, 0.5],\n",
    "      [0, 0.5],\n",
    "   ]\n",
    "))\n",
    "example_nn.fc2.weight = preset_fc2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180c83c2",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Note that our neural net expects input images to be in batches. So for example,\n",
    "you can't pass a single n x m image to the neural net, but must always turn it\n",
    "into a b x n x m tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66a8d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For example, if you try to directly call our neural net on a single image, you\n",
    "# will get an error.\n",
    "try:\n",
    "  example_nn(input_0)\n",
    "except AssertionError as e:\n",
    "  print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794b10dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Instead we can use unsqueeze to turn this image into a one image batch.\n",
    "example_nn(input_0.unsqueeze(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a992280",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We can also pass all three images at the same time.\n",
    "three_images_stacked = torch.stack([input_0, input_1, input_2])\n",
    "\n",
    "example_nn(three_images_stacked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a2ddbd",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Now let's use `visualize_ith_key_value` to visualize the first and second key\n",
    "value pairs of our small neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80964975",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# First key-value pair\n",
    "visualize_ith_key_value(\n",
    "  model=example_nn, \n",
    "  i=0, \n",
    "  key_x_size=2, \n",
    "  key_y_size=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f589928a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Second key-value pair\n",
    "visualize_ith_key_value(\n",
    "  model=example_nn, \n",
    "  i=1, \n",
    "  key_x_size=2, \n",
    "  key_y_size=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d647262d",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "*Exercise*: Verify that the results of the above two visualizations match the\n",
    "visualization of the two key-value pairs from the slides. Also verify that the\n",
    "results of passing `three_images_stacked` to the neural net match what you would\n",
    "expect given the key-value interpretation as laid out in the slides."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fde568b",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "You should run the following code block, but you do not need to actually read\n",
    "it. It's stuff that's relevant for actually training our neural nets, which we\n",
    "will not be doing today, as for time purposes we will provide you with\n",
    "pretrained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43df4af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# None of this is code that you will need to write, but you should read this\n",
    "# over to understand the structure of what kind of nets we'll be training.\n",
    "#\n",
    "# Note that we only train with 10,000 images out of the 60,000 image dataset!\n",
    "# Originally this was because I was hoping to demonstrate some interesting\n",
    "# double descent phenomena, but unfortunately I ran out of time to do that :(.\n",
    "# Nonetheless, as we'll see, 10,000 images in the train set is actually enough\n",
    "# to get to a very well trained neural net!\n",
    "\n",
    "# hyper-params\n",
    "BATCH_SIZE = 512\n",
    "TRAIN_SET_SIZE = 10000\n",
    "HIDDEN_DIM = 256\n",
    "EPOCHS = 5000\n",
    "LR = 1e-3\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# If you set this to True then this will train all the models from scratch,\n",
    "# otherwise it will look for pre-saved weights and load those instead\n",
    "TRAIN_FROM_SCRATCH = False\n",
    "# When training, should we load the entire image set into GPU memory\n",
    "LOAD_EVERYTHING_INTO_GPU_MEMORY = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba338f1",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "This next code block is worth reading a little. The main thing we should realize\n",
    "here is that we are using the MNIST dataset of handwritten digits, and that we\n",
    "have 14 different pretrained models of varying sizes (from hidden layer sizes of\n",
    "8 to 131072) we can use.\n",
    "\n",
    "As part of the bonus exercises, you can use different sized models. We will\n",
    "mainly be using the model with hidden layer size 65536, as this strikes a nice\n",
    "balance between a sufficiently complicated neural net and one that is unlikely\n",
    "to cause an out of memory error on the free version of Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72839a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=10000)\n",
    "\n",
    "hidden_dims = [8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536, 131072]\n",
    "models = [SimpleNN(hidden_dim) for hidden_dim in hidden_dims]\n",
    "\n",
    "MODEL_IDX_WE_ARE_USING=13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea52c24",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Now that we have the MNIST dataset, we can examine some pictures from it. As a\n",
    "reminder, our neural net will **recognize the handwritten digits from this\n",
    "dataset and assign them a label of 0-9**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b3f4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To understand the indexing of a dataset, the first index is the image index\n",
    "# and the second index is whether it is the image data itself or the label.\n",
    "#\n",
    "# For example let's assume the 12th image in our training dataset is an image of\n",
    "# a 7. `train_dataset[12][0]` selects the 12th image's image data (which will be\n",
    "# some 1x28x28 tensor). `train_dataset[12][1]` selects the 12th image's label\n",
    "# data (which will be a vector 10 elements long with a 1 at the 8th index (7 + 1\n",
    "# for zero indexing) and a 0 everywhere else).\n",
    "image_in_training_set = train_dataset[0][0].cpu()\n",
    "\n",
    "# We need to squeeze because MNIST by default has images with 4 dimensions:\n",
    "# batch, color channel, height, and width. The color channel is always 1 since\n",
    "# all the images are grayscale, so we can simply `squeeze` that dimension away.\n",
    "visualize_image(image_in_training_set.squeeze())\n",
    "\n",
    "# The same kind of images are used in the test dataset, we just don't\n",
    "image_in_test_dataset = test_dataset[1][0].cpu()\n",
    "visualize_image(image_in_test_dataset.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcf858e",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Once more, you should run the following code block, but you do not need to actually read\n",
    "it. This code block can be used to either execute the training loop of the model\n",
    "or pre-load its weights. We're interested mainly in the latter for today, but if\n",
    "you wanted to train these models from scratch, the code is provided for you.\n",
    "\n",
    "To reiterate, even though you don't need to read this code block, it is vital to\n",
    "run it! Otherwise you won't have pre-trained models, but rather models set to\n",
    "random parameters.\n",
    "\n",
    "Feel free to collapse and hide this code block after running if it's too distracting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0903c7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This is code that you can read if you'd like, but can also just run. It's\n",
    "# mainly useful if you wanted to train these models yourself.\n",
    "\n",
    "if LOAD_EVERYTHING_INTO_GPU_MEMORY:\n",
    "  # We'll load into memory to make this faster\n",
    "  train_loader_with_entire_dataset = torch.utils.data.DataLoader(train_dataset, batch_size=train_dataset.data.shape[0])\n",
    "  for batch_idx, (data, target) in enumerate(train_loader_with_entire_dataset):\n",
    "      data = data[:TRAIN_SET_SIZE].to(DEVICE)\n",
    "      target = torch.nn.functional.one_hot(target[:TRAIN_SET_SIZE], num_classes=10).to(torch.float)\n",
    "      target = target.to(DEVICE)\n",
    "\n",
    "  train_dataset = torch.utils.data.TensorDataset(data, target)\n",
    "  train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "  test_loader_with_entire_dataset = torch.utils.data.DataLoader(test_dataset, batch_size=test_dataset.data.shape[0])\n",
    "  for test_data, test_target in test_loader_with_entire_dataset:\n",
    "      test_data = test_data.to(DEVICE)\n",
    "      test_target = torch.nn.functional.one_hot(test_target, num_classes=10).to(torch.float)\n",
    "      test_target = test_target.to(DEVICE)\n",
    "\n",
    "\n",
    "  test_dataset = torch.utils.data.TensorDataset(test_data, test_target)\n",
    "  test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=10000)\n",
    "\n",
    "  # PyTorch DataLoader seems absurdly slow for MNIST dataset sizes\n",
    "  # It seems to be calling get_item one by one instead of doing batch operations\n",
    "  # Let's just do a custom list instead\n",
    "  def generate_simple_loader(dataset, batch_size):\n",
    "    permuted_indices = torch.randperm(dataset.tensors[0].shape[0])\n",
    "    permuted_data = dataset.tensors[0][permuted_indices]\n",
    "    permuted_target = dataset.tensors[1][permuted_indices]\n",
    "    simple_loader = []\n",
    "    for i in range(0, permuted_data.shape[0], batch_size):\n",
    "      simple_loader.append((permuted_data[i:i+batch_size], permuted_target[i:i+batch_size]))\n",
    "    return simple_loader\n",
    "\n",
    "  simple_train_loader = generate_simple_loader(train_dataset, BATCH_SIZE)\n",
    "  simple_test_loader = generate_simple_loader(test_dataset, 10000)\n",
    "\n",
    "  train_loader = simple_train_loader\n",
    "  test_loader = simple_test_loader\n",
    "\n",
    "# This is the actual training loop! Even though this is not code you will need\n",
    "# to write, you should definitely read this! It's good to understand exactly how\n",
    "# our model is being trained!\n",
    "#\n",
    "# You might notice that we're using MSELoss instead of cross-entropy loss. It\n",
    "# turns out that this is enough to get quite reasonable models and considerably\n",
    "# simplifies some of the presentataion to people who have only an introductory\n",
    "# understanding of neural nets.\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "for model in models:\n",
    "    print(f\"Processing hidden_dim {model.hidden_dim}\")\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "    # Training\n",
    "    train_loss = 0\n",
    "    train_accuracy = 0\n",
    "    train_samples = 0\n",
    "    if TRAIN_FROM_SCRATCH:\n",
    "      model = model.to(DEVICE)\n",
    "      for epoch in range(EPOCHS):\n",
    "          if LOAD_EVERYTHING_INTO_GPU_MEMORY:\n",
    "              # Re-shuffle the train loader\n",
    "              train_loader = generate_simple_loader(train_dataset, BATCH_SIZE)\n",
    "          for batch_idx, (data, target) in enumerate(train_loader):\n",
    "              optimizer.zero_grad()\n",
    "              output = model(data)\n",
    "              loss = criterion(output, target)\n",
    "              loss.backward()\n",
    "              optimizer.step()\n",
    "    else:\n",
    "      model.load_state_dict(torch.load(f\"mnist_model_hidden_layer_{model.hidden_dim}\"))\n",
    "      model = model.to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "      for data, target in train_loader:\n",
    "          output = model(data)\n",
    "          train_loss += criterion(output, target).item()\n",
    "          train_accuracy += (output.argmax(dim=1) == target.argmax(dim=1)).sum().item()\n",
    "          train_samples += data.shape[0]\n",
    "    train_losses.append(train_loss / len(train_loader))\n",
    "    train_accuracies.append(train_accuracy / train_samples)\n",
    "\n",
    "    # Testing\n",
    "    test_loss = 0\n",
    "    test_accuracy = 0\n",
    "    test_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for test_data, test_target in test_loader:\n",
    "            output = model(test_data)\n",
    "            loss = criterion(output, test_target)\n",
    "            test_loss += loss.item()\n",
    "            test_accuracy += (output.argmax(dim=1) == test_target.argmax(dim=1)).sum().item()\n",
    "            test_samples += test_data.shape[0]\n",
    "    test_losses.append(test_loss / len(test_loader))\n",
    "    test_accuracies.append(test_accuracy / test_samples)\n",
    "\n",
    "plt.plot(hidden_dims, train_losses, label='Train Loss')\n",
    "plt.plot(hidden_dims, test_losses, label='Test Loss')\n",
    "plt.xlabel('Hidden Dimension')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss vs Hidden Dim')\n",
    "plt.xscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(hidden_dims, train_accuracies, label='Train Accuracy')\n",
    "plt.plot(hidden_dims, test_accuracies, label='Test Accuracy')\n",
    "plt.xlabel('Hidden Dimension')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy vs Hidden Dim')\n",
    "plt.xscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "if TRAIN_FROM_SCRATCH:\n",
    "  for dim, model in zip(hidden_dims, models):\n",
    "    # Save on CPU because this makes it easier to load for more devices\n",
    "    model = model.to(\"cpu\")\n",
    "    torch.save(model.state_dict(), f\"mnist_model_hidden_layer_{dim}\")\n",
    "\n",
    "# Go ahead and run this just to make sure we're on the correct device\n",
    "\n",
    "for model in models:\n",
    "  model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcd164d",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "As a reminder we're mainly only the `MODEL_IDX_WE_ARE_USING`-th model for most\n",
    "of our exercises today.\n",
    "\n",
    "Let's measure how accurately this model classifies digits. Accuracy will be\n",
    "measured as a 10 element list, where the $i-1$-ith member of the list (since the\n",
    "list is zero indexed) corresponds to the model's accuracy at correctly\n",
    "identifying images of the $i$-th digit.\n",
    "\n",
    "For example, `[0.9, 0.8, 0.7, 0.6, 0.5, 0.55, 0.65, 0.75, 0.85, 0.95]` means\n",
    "that the model correctly identifies images of the digit 3 as 3 60% of the time\n",
    "(since the fourth element of the list is 0.6).\n",
    "\n",
    "You should notice that most of the accuracy scores are in the high nineties for\n",
    "our model.\n",
    "\n",
    "By the end of these exercises, you will see how to selectively drive down the\n",
    "model's ability to recognize one digit down to near-zero accuracy with minimal\n",
    "interference for other digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7b4f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# calculates the accuracy of the model for each digit.\n",
    "\n",
    "def accuracy_by_digit(model, loader):\n",
    "  correct = [0] * 10\n",
    "  total = [0] * 10\n",
    "  with torch.no_grad():\n",
    "    for data, target_probs in loader:\n",
    "      output_probs = model(data)\n",
    "      output = output_probs.argmax(dim=1)\n",
    "      target = target_probs.argmax(dim=1)\n",
    "      for i in range(target.shape[0]):\n",
    "        total[target[i]] += 1\n",
    "        if output[i] == target[i]:\n",
    "          correct[target[i]] += 1\n",
    "  return [correct[i] / total[i] for i in range(10)]\n",
    "\n",
    "# Test it out for our chosen model\n",
    "accuracy_by_digit(models[MODEL_IDX_WE_ARE_USING], test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256258fc",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "We can compare this accuracy against our smallest model, which is substantially\n",
    "less accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bddf63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test it out for our 8 hidden units model\n",
    "accuracy_by_digit(models[0], test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e096fc5",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "We can now begin looking at much larger vanilla neural nets than what we were\n",
    "looking at before. Using our 65k hidden unit model, let's look at one\n",
    "interesting key-value pair, the 463rd one.\n",
    "\n",
    "*Exercise*: For the 463rd key-value pair, given the representation of the key\n",
    "below, what do you think the value vector looks like? In particular, which index\n",
    "of the value vector do you think has the highest value? Remember that red in the\n",
    "key visualization indicates a high positive value and blue indicates a high\n",
    "negative value, while white indicates a zero value.\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "You should see that the image looks roughly like a circle, which indicates that\n",
    "this is a key that recognizes 0s. This in turn means that the value vector\n",
    "should likely write out most highly at the 0th position, and pretty low\n",
    "elsewhere.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcb2e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "INTERESTING_KEY_VALUE_PAIR = 463\n",
    "\n",
    "visualize_ith_key(models[MODEL_IDX_WE_ARE_USING].cpu(), INTERESTING_KEY_VALUE_PAIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6848bdda",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "You can verify the solution to the exercise by printing the actual value vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49680cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_ith_value(models[MODEL_IDX_WE_ARE_USING].cpu(), INTERESTING_KEY_VALUE_PAIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98b94bb",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "You can also visualize both the key and the value at the same time (which is\n",
    "what we did for the simpler neural net on 2x2 images earlier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c67e74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_ith_key_value(models[MODEL_IDX_WE_ARE_USING].cpu(), INTERESTING_KEY_VALUE_PAIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d7579e",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "What is a more general way we could find interesting key-value pairs to look at?\n",
    "Well we could look for those key-value pairs which selectively recognize one\n",
    "particular kind of digit and not others.\n",
    "\n",
    "*Exercise*: What kind of key and value vectors would be expect to see for\n",
    "*key-value pairs which selectively recognize one digit and not others? Would we\n",
    "*pay more attention to the key vector or the value vector?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "For this it's generally more useful to look at the value vector, since this\n",
    "provides the answer as to what the key-value pair \"thinks\" a given image is. In\n",
    "particular, we would expect the value vector to have a high value for one index\n",
    "and a low value at all other indices.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222d2e57",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "The following function is one very simple operationalization of the previous\n",
    "exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77dba49",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "# This is a very rough-and-tumble function. We pass in the digit we're\n",
    "# interested in and then just look at how high of a value the value vector has\n",
    "# at the corresponding index and then how low the absolute values of all the other\n",
    "# values are. If the value corresponding to the digit is high (i.e. over\n",
    "# `digit_threshold`) and the values corresponding to the other digits are low\n",
    "# (i.e. less than `other_digits_threshold`), then we keep that result.\n",
    "\n",
    "def find_values_for_sole_digit(model, digit, digit_threshold=0.16, other_digits_threshold=0.07):\n",
    "  result = []\n",
    "  for idx in range(model.fc2.weight.shape[1]):\n",
    "    other_digits = torch.abs(model.fc2.weight[:, idx])\n",
    "    other_digits[digit] = 0\n",
    "    max_of_other_digits = torch.max(other_digits)\n",
    "    if max_of_other_digits.item() > other_digits_threshold:\n",
    "      continue\n",
    "    elif model.fc2.weight[digit, idx] > digit_threshold:\n",
    "      result.append(idx)\n",
    "  return torch.tensor(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d69edfb",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Let's use `find_values_for_sole_digit` to find interesting key-value pairs that\n",
    "very selectively recognize handwritten digits of 1s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73776695",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_values_for_sole_digit(models[MODEL_IDX_WE_ARE_USING], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5c2b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should find from the previous block that 7568 is one of the key-value\n",
    "# pairs that very selectively recognizes 1s. Let's visualize that.\n",
    "\n",
    "visualize_ith_key_value(models[MODEL_IDX_WE_ARE_USING].cpu(), 7568)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01dac32",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "*Exercise*: Use the next code block to call `find_values_for_sole_digit` to find\n",
    "interesting key-value pairs that very selectively recognize handwritten digits\n",
    "of 0s. Do the keys actually look like zeros?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24324886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill in this code block with the proper call to\n",
    "# find_values_for_sole_digit and then visualize each of the key-value pairs from\n",
    "# the call!\n",
    "\n",
    "# raise NotImplementedError(\"\")\n",
    "\n",
    "#BEGIN SOLUTION\n",
    "digit_to_analyze = 0\n",
    "\n",
    "indices_that_fire_mainly_on_select_digit = find_values_for_sole_digit(models[MODEL_IDX_WE_ARE_USING], digit_to_analyze)\n",
    "for idx in indices_that_fire_mainly_on_select_digit[:3]:\n",
    "  visualize_ith_key_value(models[MODEL_IDX_WE_ARE_USING].to(\"cpu\"), idx)\n",
    "#END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234e9908",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "We've been able to identify interesting key-value pairs, but we have no\n",
    "guarantee that those key-value pairs are actually the most important ones that\n",
    "the model uses to do image recognition.\n",
    "\n",
    "This is an important problem within mechanistic interpretability: how do we\n",
    "*attribute* certain kinds of behaviors to certain structures within the neural\n",
    "net?\n",
    "\n",
    "In a certain sense, starting with interesting structures is a bit backwards,\n",
    "since have no idea if those structures actually are the main way that the model\n",
    "recognizes handwritten digits or if they're just mostly unused anomalies.\n",
    "\n",
    "We won't have time today to thoroughly explore attribution, other than to\n",
    "mention that it's tricky! Even for a vanilla neural net it can be non-obvious\n",
    "how to actually robustly perform attribution.\n",
    "\n",
    "For now, we'll just look at some individual images and try to do some very rough\n",
    "attribution on the fly for each of them. Let's begin with an image from the\n",
    "model's training set, an image of a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ffd74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We explored this earlier, but we'll copy this again from before.\n",
    "#\n",
    "# To understand the indexing of a dataset, the first index is the image index\n",
    "# and the second index is whether it is the image data itself or the label.\n",
    "#\n",
    "# For example let's assume the 12th image in our training dataset is an image of\n",
    "# a 7. `train_dataset[12][0]` selects the 12th image's image data (which will be\n",
    "# some 1x28x28 tensor). `train_dataset[12][1]` selects the 12th image's label\n",
    "# data (which will be a vector 10 elements long with a 1 at the 8th index (7 + 1\n",
    "# for zero indexing) and a 0 everywhere else).\n",
    "image_of_zero_in_training_set = train_dataset[1][0].cpu() # I just happen to know that the 1-th element is an image of a 0\n",
    "\n",
    "# We need to squeeze because MNIST by default has images with 4 dimensions:\n",
    "# batch, color channel, height, and width. The color channel is always 1 since\n",
    "# all the images are grayscale, so we can simply `squeeze` that dimension away.\n",
    "visualize_image(image_of_zero_in_training_set.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101b101e",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "*Exercise*: Visualize the image of the 10th picture in our training set\n",
    "(including zero indexing, so you should be using 10 as index for the dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c9241f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: First correctly index `train_dataset`, and then call `visualize_image`\n",
    "# raise NotImplementedError(\"\")\n",
    "\n",
    "#BEGIN SOLUTION\n",
    "visualize_image(train_dataset[10][0].cpu().squeeze())\n",
    "#END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f383724e",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "This next block of code attempts to perform some very rough attribution\n",
    "\n",
    "It's a significant amount of code, but it's not necessary to read all of it to\n",
    "understand what it's doing. Feel free to collapse this cell after running it.\n",
    "\n",
    "It all culminates in `list_top_kv_pair_idxs`, which is a function that finds\n",
    "those key-value pairs which \"activate\" the most on a given image. We measure\n",
    "activation of a key-value pair by looking at the sum of the absolute values of\n",
    "the final scaled value vector written by the key-value pair. \n",
    "\n",
    "A high number indicates that we have a key-value pair that potentially was very\n",
    "instrumental in helping the neural net identify the digit. A value close to zero\n",
    "indicates that that key-value pair had very little impact on the neural net's\n",
    "evaluation of that image.\n",
    "\n",
    "`list_top_kv_pair_idxs` gives back a 1-d tensor of those key-value pairs which\n",
    "contributed the most according to this simple activation definition to the\n",
    "neural net's evaluation of a particular image, sorted by activation amount.\n",
    "\n",
    "`excess_abs_weight` is a parameter that controls how many key-value pairs we\n",
    "ignore and do not return in the final result. So for example an\n",
    "excess_abs_weight of 1000 means that we discard all the key-value pairs with the\n",
    "lowest activation scores such that their total sum does not exceed 1000.\n",
    "\n",
    "Higher values of `excess_abs_weight` will decrease the size of the tensor that\n",
    "is returned because more key-value pairs will be pruned out. \n",
    "\n",
    "If you require very high levels (e.g. > 2000) of `excess_abs_weight` to prune out key-value\n",
    "pairs, this generally indicates that the model is spreading most of its\n",
    "evaluation across many different key-value pairs.\n",
    "\n",
    "Low levels of `excess_abs_weight` which prune out most key-value pairs on the\n",
    "other hand indicate that for that particular image, the model evaluates it\n",
    "mainly just using a few key-value pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a01f3c2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "# This function will give us the internal outputs of all the keys and values for\n",
    "# a given image. In other words this will return the dot product of each key\n",
    "# with the image (combined with the bias per key) and will also return the\n",
    "# scaled value vector.\n",
    "#\n",
    "# If this is confusing to you, it may be helpful to go back to the slides and\n",
    "# look a little bit more at the break-down of how exactly we calculate a neural\n",
    "# net's output using the key-value paradigm.\n",
    "\n",
    "def compute_kv_outputs_for_image(model, input_image):\n",
    "  flattened_img = model.flatten(input_image)\n",
    "  output_after_keys = model.fc1(flattened_img)\n",
    "  output_after_relu = model.relu(output_after_keys)\n",
    "  # We ultimately want to multiple all the components of each value vector by\n",
    "  # the same value, so we need to do a repeat first and then we can do a\n",
    "  # standard element-wise tensor multiplication\n",
    "  #\n",
    "  # But this is just the same as broadcasting, so we just use that instead\n",
    "  output_after_values = model.fc2.weight * output_after_relu\n",
    "  return output_after_keys, output_after_values\n",
    "\n",
    "def top_indices_by_tail_sum(tensor: torch.Tensor, threshold: float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Given a 1D tensor and a threshold, returns the indices of the largest values\n",
    "    such that the sum of all smaller values (i.e. the “tail” after that point)\n",
    "    is <= threshold.\n",
    "    \"\"\"\n",
    "    assert tensor.dim() == 1, \"Only works on 1D tensors\"\n",
    "    # Sort descending\n",
    "    sorted_vals, sorted_idx = tensor.sort(descending=True)\n",
    "    # Compute cumulative sum of the sorted values\n",
    "    cumsum = sorted_vals.cumsum(dim=0)\n",
    "    total = cumsum[-1]\n",
    "    # tail_sums[i] = sum(sorted_vals[i+1:])\n",
    "    tail_sums = total - cumsum\n",
    "    # find the first position where tail_sums <= threshold\n",
    "    mask = tail_sums <= threshold\n",
    "    if not mask.any():\n",
    "        # no cutoff—tail never drops below threshold, so return empty\n",
    "        return torch.empty(0, dtype=torch.long)\n",
    "    cutoff = mask.nonzero(as_tuple=False)[0].item()\n",
    "    return sorted_idx[:cutoff]\n",
    "\n",
    "# Example\n",
    "x = torch.tensor([1, 4, 2, 3, 1], dtype=torch.float)\n",
    "indices = top_indices_by_tail_sum(x, threshold=4)\n",
    "# print(f\"{indices=}\")  # tensor([1, 3])\n",
    "\n",
    "# Let's prove to ourselves that the key-value paradigm of calculating things is equal to the normal layer-by-layer interpretation\n",
    "def sanity_check_kv_outputs(model, input_image):\n",
    "  _, output_after_values = compute_kv_outputs_for_image(model, input_image)\n",
    "  output_plus_bias = einops.einsum(output_after_values, \"digits num_of_values -> digits\") + model.fc2.bias\n",
    "  # Uncomment these if you want to actually see the sanity check\n",
    "  # print(f\"{output_plus_bias.softmax(dim=-1)=}\")\n",
    "  # print(f\"{model(input_image)=}\")\n",
    "\n",
    "# You should see that the two print statements print the same values\n",
    "sanity_check_kv_outputs(models[MODEL_IDX_WE_ARE_USING], train_dataset[0][0].cpu())\n",
    "\n",
    "#returns the most influential key-value pairs for an image\n",
    "def list_top_kv_pair_idxs(model, input_image, excess_abs_weight=500):\n",
    "  _, output_after_values = compute_kv_outputs_for_image(model, input_image)\n",
    "  abs_values = einops.einsum(torch.abs(output_after_values), \"digits num_of_values -> num_of_values\")\n",
    "  indices = top_indices_by_tail_sum(abs_values, excess_abs_weight)\n",
    "  return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ef6c7a",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Now let's find those key-value pairs that activate most strongly for the image of a zero we saw a while ago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b376cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will list the key-value pairs that write the value vectors with the largest magnitude.\n",
    "top_key_value_pairs_for_img_of_zero = list_top_kv_pair_idxs(models[13], image_of_zero_in_training_set, 1800)\n",
    "top_key_value_pairs_for_img_of_zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5a566e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 42138 is the key-value pair with the highest activation for this particular image\n",
    "first_highest_activation_kv = top_key_value_pairs_for_img_of_zero[0]\n",
    "print(f\"{first_highest_activation_kv=}\")\n",
    "visualize_ith_key_value_on_image(models[MODEL_IDX_WE_ARE_USING], first_highest_activation_kv, image_of_zero_in_training_set.squeeze())\n",
    "visualize_image(image_of_zero_in_training_set.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a41d2d4",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "*Exercise*: Visualize the 2nd and 3rd most activating key-value pairs for\n",
    "`image_of_zero_in_training_set`. Do you notice anything interesting about them?\n",
    "In particular, do they seem to select for digits that aren't just 0? If so,\n",
    "which ones and does it make sense that they are selecting for those as well?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "You should notice that all these highly fire for 2s as well. This makes sense\n",
    "because the part of the 2 that is not the bottom horizontal line has a lot of\n",
    "overlap with 0.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a376d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill in code block with correct calls to visualize the 2nd and 3rd most\n",
    "# activating key pairs for `image_of_zero_in_training_set`\n",
    "\n",
    "# raise NotImplementedError()\n",
    "#BEGIN SOLUTION\n",
    "second_highest_activation_kv = top_key_value_pairs_for_img_of_zero[1]\n",
    "print(f\"{second_highest_activation_kv=}\")\n",
    "visualize_ith_key_value_on_image(models[MODEL_IDX_WE_ARE_USING], second_highest_activation_kv, image_of_zero_in_training_set.squeeze())\n",
    "visualize_image(image_of_zero_in_training_set.squeeze())\n",
    "\n",
    "third_highest_activation_kv = top_key_value_pairs_for_img_of_zero[2]\n",
    "print(f\"{third_highest_activation_kv=}\")\n",
    "visualize_ith_key_value_on_image(models[MODEL_IDX_WE_ARE_USING], third_highest_activation_kv, image_of_zero_in_training_set.squeeze())\n",
    "visualize_image(image_of_zero_in_training_set.squeeze())\n",
    "#END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed8c0d6",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "`top_key_value_pairs_for_img_of_zero` gives us a basic way of doing attribution,\n",
    "but we're still missing a crucial piece in how we perform attribution, namely\n",
    "the ability to see how the network performs when we only use certain key-value\n",
    "pairs (or equivalently when we omit certain ones).\n",
    "\n",
    "This lets us get more confidence we understand how the network is recognizing\n",
    "images by letting us see if the removal of what we think are relevant neurons\n",
    "actually removes the behavior we are analyzing or whether their inclusion\n",
    "maintains the behavior.\n",
    "\n",
    "The next function is developed for this purpose. It returns both logits (raw\n",
    "scores that are not softmax-ed) and the softmax-ed probabilities. The latter is\n",
    "technically a bit harder to understand than the former, because it doesn't\n",
    "compose as nicely. The logits of different key-value pairs are just summed\n",
    "together by the neural net to get the final answer, but softmax is a non-linear\n",
    "function that makes attribution a little murkier.\n",
    "\n",
    "We include the softmax-ed results nonetheless because they are useful for\n",
    "building intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc7fa0c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "#function to find the logits and probabilities if the model only uses a certain set of key-value pair indices\n",
    "def calculate_output_only_with_certain_kv_indices(model, img, index_list):\n",
    "\n",
    "  image = img.unsqueeze(0)\n",
    "  image = model.flatten(img)\n",
    "  index_list = torch.tensor(index_list)\n",
    "\n",
    "  all_outputs = model.fc1(image)\n",
    "  all = model.relu(all_outputs)\n",
    "\n",
    "  zeroing = torch.zeros_like(all[0])\n",
    "  zeroing[index_list] = 1.0\n",
    "\n",
    "  all = all * zeroing\n",
    "\n",
    "  logits = model.fc2(all)\n",
    "  probabilities = model.softmax(logits)\n",
    "\n",
    "  return logits, probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18ed57b",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "With this function in hand, we can verify that in fact if we restrict the\n",
    "network to just using the top key-value pairs we found previously, it does still\n",
    "recognize the image as a zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3dd60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should notice that even with just the 13 key-value pairs in\n",
    "# `top_key_value_pairs_for_img_of_zero`, our network is already able to quite\n",
    "# confidently recognize this digit as a 0.\n",
    "calculate_output_only_with_certain_kv_indices(\n",
    "  models[MODEL_IDX_WE_ARE_USING], \n",
    "  image_of_zero_in_training_set,\n",
    "  top_key_value_pairs_for_img_of_zero,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0798cf88",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "But you might notice a bit of an oddity here. The top 3 highest activating\n",
    "key-value pairs for this image of a 0 actually also highly activate for 2. In\n",
    "fact several of them write a value vector with values higher for a 2 than a 0!\n",
    "\n",
    "What's going on here? Let's explore that a little. First, we can notice that if\n",
    "we only include the top 5 highest activating key-value pairs, the neural net\n",
    "thinks that the image is more of a 2 than a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30139f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_output_only_with_certain_kv_indices(\n",
    "  models[MODEL_IDX_WE_ARE_USING], \n",
    "  image_of_zero_in_training_set,\n",
    "  top_key_value_pairs_for_img_of_zero[:5],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dfbccc",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "So that means somewhere between the top 5 highest activating key-value pairs and\n",
    "the remaining 8 highest activating key-value pairs (because there are a total of\n",
    "13 highest activatig key-value pairs we're using), the model \"flipped\" and\n",
    "decided that the image was much of a 0 than a 2.\n",
    "\n",
    "Let's visualize each of the remaining key-value pairs to see why that happened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a93735",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in top_key_value_pairs_for_img_of_zero[5:]:\n",
    "  visualize_ith_key_value_on_image(models[MODEL_IDX_WE_ARE_USING], i, image_of_zero_in_training_set.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823e6473",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "*Exercise*: Using these visualizations, and any others you see fit to use, can\n",
    "you come up with an explanation for how the model decides that this is more of a\n",
    "0 than a 2? Can you identify specific key-value pairs/neurons that are\n",
    "responsible for this behavior?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "There's some variation in possible answers here, but you might notice after some\n",
    "exploration that really what's causing the neural net to \"hesitate\" between a 0\n",
    "and a 2, is the presence of keys that activate heavily on the lower part of a 0,\n",
    "which could correspond either to the bottom of a zero, or to the bottom\n",
    "horizontal line of a 2.\n",
    "\n",
    "If you look at some of the other highly-activating key-value pairs, you should\n",
    "find that some of them heavily favor 0 over 2, because the keys activate a lot\n",
    "less in that region (and don't have something that looks closer to a horizontal\n",
    "line).\n",
    "\n",
    "So one version of an explanation could go: the neural net ultimately tie-breaks\n",
    "between 0 and 2 by using a lot of keys which do not have very \"long horizontal\"\n",
    "streaks at the bottom of a 0 digit to select for the 0 over a 2.\n",
    "\n",
    "Feel free to look in the solutions file for examples of some scratch code I\n",
    "wrote to come to this conclusion.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f3ddd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function might come in handy (but you don't have to use it). It shows you\n",
    "# the specific key and value activation results for a certain key-value pair\n",
    "# when applied to a single image.\n",
    "#\n",
    "# You can also just manually inspect each of the 13 key-value pairs yourself.\n",
    "def calculate_kv_activation_for_specific_kv(model, img, kv_idx):\n",
    "  keys, values = compute_kv_outputs_for_image(model, img)\n",
    "  return keys[:, kv_idx], values[:, kv_idx]\n",
    "\n",
    "# TODO: Scratch space for any code you want to write to come up with an explanation.\n",
    "\n",
    "# The following is one example of some code you might write\n",
    "key_value_indices_that_prefer_0_over_2 = []\n",
    "for kv_pair_idx in top_key_value_pairs_for_img_of_zero[5:]:\n",
    "  _, values = calculate_kv_activation_for_specific_kv(models[MODEL_IDX_WE_ARE_USING], image_of_zero_in_training_set, kv_pair_idx)\n",
    "  if values[0] > values[2]:\n",
    "    key_value_indices_that_prefer_0_over_2.append(kv_pair_idx)\n",
    "\n",
    "# Let's visualize all of the key-value pairs that prefer 0 over 2.\n",
    "for kv_pair_idx in key_value_indices_that_prefer_0_over_2:\n",
    "  visualize_ith_key_value_on_image(models[MODEL_IDX_WE_ARE_USING], kv_pair_idx, image_of_zero_in_training_set.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaedf03",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "At this point we have some idea of how the model is able to recognize our chosen\n",
    "image as a 0. It has keys that fire heavily on the curves of the 0, especially\n",
    "the right-hand curve, and then uses some additional keys that don't have a\n",
    "\"bottom curve\" component to disambiguate between 0 and 2.\n",
    "\n",
    "Let's move on to an image from the training data set that is more difficult to\n",
    "interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c64506a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_of_one_in_training_set = train_dataset[3][0].cpu()\n",
    "visualize_image(image_of_one_in_training_set.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ea8865",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Let's look again at the top key-value pairs. Right off the bat, you should\n",
    "notice we have a *lot* more.\n",
    "\n",
    "This already indicates that we're going to have a tougher time interpreting\n",
    "what's going on. Instead of being able to confine our attention to a handful of\n",
    "key-value pairs, we potentially have to do deal with way more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31a1bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_key_value_pairs_for_img_of_one = list_top_kv_pair_idxs(models[13], image_of_one_in_training_set, 1800)\n",
    "top_key_value_pairs_for_img_of_one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be47ca3",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Let's do a quick sanity check to make sure that when we confine the neural net\n",
    "to just using these top key-value pairs, it still recognizes this image as a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0967cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_output_only_with_certain_kv_indices(\n",
    "  models[MODEL_IDX_WE_ARE_USING], \n",
    "  image_of_one_in_training_set,\n",
    "  top_key_value_pairs_for_img_of_one,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4711bb",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Now let's look at one of the particular key-value pairs. This one is really\n",
    "strange. It's the 6th highest (remember zero-indexing, so we use 5 to index into\n",
    "the list) activating key-value pair, but hardly thinks the image is a 1 at all!\n",
    "Instead it most strongly thinks it's an 8, and also might be a 2 or a 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27d2366",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_ith_key_value_on_image(\n",
    "  models[MODEL_IDX_WE_ARE_USING], \n",
    "  top_key_value_pairs_for_img_of_one[5], \n",
    "  image_of_one_in_training_set.squeeze()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9db743",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Indeed if we look at the top 10 highest activating key-value pairs, taking\n",
    "together, they think that the image is probably a 2 or an 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00344d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_output_only_with_certain_kv_indices(\n",
    "  models[MODEL_IDX_WE_ARE_USING], \n",
    "  image_of_one_in_training_set,\n",
    "  top_key_value_pairs_for_img_of_one[:10],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfc1f95",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Only when you expand out to around the top 44 or so, then finally, begrudgingly,\n",
    "the model thinks that the image is likely a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b53d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_output_only_with_certain_kv_indices(\n",
    "  models[MODEL_IDX_WE_ARE_USING], \n",
    "  image_of_one_in_training_set,\n",
    "  top_key_value_pairs_for_img_of_one[:44],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c558b3c",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "*Exercise*: Attempt to explain how the model is able to conclude that this image\n",
    "is in fact a 1. Why do you think that how the neural net is able to conclude\n",
    "that the image is a 1 is far more messy than for a 0?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "The explanation here is a lot more tentative than for our previous image. You\n",
    "should retain some skepticism about this and if you have time, feel free to more\n",
    "thoroughly test this explanation!\n",
    "\n",
    "It looks like basically there's a lot of interference from other digits because\n",
    "a diagonal line (and most 1s in the dataset are written as a diagonal line\n",
    "rather than e.g. a vertical line) will intersect with many other keys (e.g. a 2\n",
    "has a digonal line in it, an 8 written in a slanted fashion basically has a\n",
    "diagonal line through it, and a 7 also has a diagonal in it).\n",
    "\n",
    "So the neural net is forced to have a much more patchwork set of keys that all\n",
    "activate on various different parts of a 1 and cancel out on various other parts\n",
    "of other digits to piece them together.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e81dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Scratch space for any code you want to write to come up with an explanation.\n",
    "# raise NotImplementedError()\n",
    "\n",
    "#BEGIN SOLUTION\n",
    "# Let's see which key-value pairs prefer 1.\n",
    "key_value_indices_that_prefer_1 = []\n",
    "for kv_pair_idx in top_key_value_pairs_for_img_of_one:\n",
    "  _, values = calculate_kv_activation_for_specific_kv(models[MODEL_IDX_WE_ARE_USING], image_of_one_in_training_set, kv_pair_idx)\n",
    "  if values.argmax() == 1:\n",
    "    key_value_indices_that_prefer_1.append(kv_pair_idx)\n",
    "print(f\"{key_value_indices_that_prefer_1=}\")\n",
    "for idx in key_value_indices_that_prefer_1[:5]:\n",
    "  visualize_ith_key_value_on_image(\n",
    "    models[MODEL_IDX_WE_ARE_USING], \n",
    "    idx, \n",
    "    image_of_one_in_training_set.squeeze()\n",
    "  )\n",
    "\n",
    "# To get an idea of the interference we're getting, look at the those kv-pairs\n",
    "# which think that the image is more of a 7 than a 1\n",
    "key_value_indices_that_prefer_7_over_1 = []\n",
    "for kv_pair_idx in top_key_value_pairs_for_img_of_one:\n",
    "  _, values = calculate_kv_activation_for_specific_kv(models[MODEL_IDX_WE_ARE_USING], image_of_one_in_training_set, kv_pair_idx)\n",
    "  if values[7] > values[1] and values[7] > 0.1:\n",
    "    key_value_indices_that_prefer_7_over_1.append(kv_pair_idx)\n",
    "\n",
    "# There's a lot of key-value pairs that prefer 7 over 1 here!\n",
    "print(f\"{key_value_indices_that_prefer_7_over_1=}\")\n",
    "\n",
    "for idx in key_value_indices_that_prefer_7_over_1[:5]:\n",
    "  visualize_ith_key_value_on_image(\n",
    "    models[MODEL_IDX_WE_ARE_USING], \n",
    "    idx, \n",
    "    image_of_one_in_training_set.squeeze()\n",
    "  )\n",
    "#END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe36025b",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "This illustrates how even for our very simple neural net, mechanistic\n",
    "interpretability can be quite difficult!\n",
    "\n",
    "Let's see if we can quantify a little bit this difficulty. If we plot all the\n",
    "positive key activations (i.e. the dot product of the key with the image + the\n",
    "bias) and sort them by key activation amount, we can see that for the image of a\n",
    "0, we have a pretty sharp curve. That is the majority of key activations are\n",
    "squeezed into a relatively small number of keys.\n",
    "\n",
    "If we compare that to the image of a 1, we see a much flatter curve, which\n",
    "suggests that the key activations are much more \"spread out\" and we have to\n",
    "understand a lot more keys to get a handle on why the image is recognized as a 1\n",
    "by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c748c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns all positive key activations (dot products + bias) for each key for an image\n",
    "def calculate_key_activation(model, img):\n",
    "    viz_img = img\n",
    "    visualize_image(viz_img)\n",
    "    dot_products_with_bias = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(model.fc1.weight.shape[0]):\n",
    "            key = model.fc1.weight[i].reshape(28, 28)\n",
    "            key_bias = model.fc1.bias[i]\n",
    "            element_wise_multi = key * img\n",
    "            dot = torch.sum(element_wise_multi)\n",
    "            dot_with_bias = dot + key_bias\n",
    "\n",
    "            if(dot > 0):\n",
    "                dot_products_with_bias.append(dot_with_bias.item())\n",
    "\n",
    "    return dot_products_with_bias\n",
    "\n",
    "#plots the key activations, ensuring the x and y axis has the same scale each time\n",
    "def plot_key_activations(dot_products_with_bias):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    if isinstance(dot_products_with_bias, torch.Tensor):\n",
    "        dot_products_with_bias = dot_products_with_bias.numpy()\n",
    "\n",
    "    sort_indices = np.argsort(dot_products_with_bias)\n",
    "    sorted_dot_products_with_bias = np.array(dot_products_with_bias)[sort_indices]\n",
    "\n",
    "    x_indices = range(len(sorted_dot_products_with_bias))\n",
    "\n",
    "    plt.plot(x_indices, sorted_dot_products_with_bias, 'r-', label='Dot Product + Key Bias', linewidth=1.5, alpha=0.7)\n",
    "\n",
    "    plt.xlabel('Keys (sorted by key activation, ascending)')\n",
    "    plt.ylabel('Key Activation')\n",
    "    plt.title('Distribution of Key Activations')\n",
    "    plt.legend()\n",
    "    plt.ylim(0, 10)\n",
    "    plt.xlim(0, 5000)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "key_activations_for_image_of_zero = calculate_key_activation(models[MODEL_IDX_WE_ARE_USING].cpu(), image_of_zero_in_training_set.squeeze())\n",
    "plot_key_activations(key_activations_for_image_of_zero)\n",
    "\n",
    "key_activations_for_image_of_one = calculate_key_activation(models[MODEL_IDX_WE_ARE_USING].cpu(), image_of_one_in_training_set.squeeze())\n",
    "plot_key_activations(key_activations_for_image_of_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a060fd96",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Now let's go and analyze an image that the model gets wrong. This time we'll use\n",
    "the test dataset to find such an image.\n",
    "\n",
    "Here's an image of a 4 that the model really really thinks is a 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9000f380",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_of_four_in_test_set = test_dataset[247][0].cpu()\n",
    "visualize_image(image_of_four_in_test_set.squeeze())\n",
    "models[MODEL_IDX_WE_ARE_USING].cpu()(image_of_four_in_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a03092",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Like before, we can use `list_top_kv_pair_idxs` to find a set of most signficant\n",
    "key-value pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a3c2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_key_value_pairs_for_img_of_four = list_top_kv_pair_idxs(models[MODEL_IDX_WE_ARE_USING].cpu(), image_of_four_in_test_set, 500)\n",
    "print(f\"{top_key_value_pairs_for_img_of_four=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475d9bca",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Let's do a quick sanity check to make sure that restricting these top key value\n",
    "pairs to an image of a four still has the model thinking that it's a 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b25666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_output_only_with_certain_kv_indices(\n",
    "  models[MODEL_IDX_WE_ARE_USING], \n",
    "  image_of_four_in_test_set,\n",
    "  top_key_value_pairs_for_img_of_four,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b6bcf0",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "This image has a pretty interesting thing where if we look at the initial group\n",
    "of top activating key-value pairs, the model initially thinks that the image is\n",
    "in fact a 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab162c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_output_only_with_certain_kv_indices(\n",
    "  models[MODEL_IDX_WE_ARE_USING], \n",
    "  image_of_four_in_test_set,\n",
    "  top_key_value_pairs_for_img_of_four[:15],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdef5761",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "*Exercise*: Why does the model think that the image is a 6 with only the first 15\n",
    "top activating key-value pairs used? What is the general shape of the\n",
    "keys that activate for this and can you make sense of this?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "The model seems to detect sixes by mainly looking at blobs in the center of the\n",
    "image with a little bit of some stuff above the blob. You can look e.g. at\n",
    "`top_key_value_pairs_for_img_of_four[14]` or other examples in the solutions\n",
    "file for an example of this.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1553ccfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Scratch space for any code you want to write to come up with an explanation.\n",
    "# raise NotImplementedError()\n",
    "\n",
    "#BEGIN SOLUTION\n",
    "# These both seem to code heavily for sixes.\n",
    "visualize_ith_key_value_on_image(\n",
    "  models[MODEL_IDX_WE_ARE_USING], \n",
    "  top_key_value_pairs_for_img_of_four[14], \n",
    "  image_of_four_in_test_set.squeeze(),\n",
    ")\n",
    "visualize_ith_key_value_on_image(\n",
    "  models[MODEL_IDX_WE_ARE_USING], \n",
    "  top_key_value_pairs_for_img_of_four[13], \n",
    "  image_of_four_in_test_set.squeeze(),\n",
    ")\n",
    "#END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd491914",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "In fact, all the way up to the first 500 most highly activating key-value pairs,\n",
    "the model still thinks that the image is likely a 6.\n",
    "\n",
    "Somewhere between about the first 500 and first 700 most highly activating\n",
    "key-value pairs, the model goes from a 6 to a 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8f2452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that if go up to the first 600, we start getting more confidence that\n",
    "# the image is a 2.\n",
    "output_of_first_600_kv_pairs = calculate_output_only_with_certain_kv_indices(\n",
    "  models[MODEL_IDX_WE_ARE_USING], \n",
    "  image_of_four_in_test_set,\n",
    "  top_key_value_pairs_for_img_of_four[:600],\n",
    ")\n",
    "print(f\"{output_of_first_600_kv_pairs=}\")\n",
    "# And that if you go to the first 700 then it is very confident that the image\n",
    "# is a 2\n",
    "output_of_first_700_kv_pairs = calculate_output_only_with_certain_kv_indices(\n",
    "  models[MODEL_IDX_WE_ARE_USING], \n",
    "  image_of_four_in_test_set,\n",
    "  top_key_value_pairs_for_img_of_four[:700],\n",
    ")\n",
    "print(f\"{output_of_first_700_kv_pairs=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f2f155",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "*Exercise*: So why does the model go from thinking that the image is a 6 to\n",
    "thinking that the image is a 2?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "The explanation here again is a lot more tentative than for our previous image. You\n",
    "should retain some skepticism about this and if you have time, feel free to more\n",
    "thoroughly test this explanation!\n",
    "\n",
    "Basically, a lot of the key value pairs that strongly make the model think that\n",
    "an image is a 2 do so with a component that looks for a strong horizontal stripe\n",
    "(the bottom horizontal line of a 2). This just happens to line up very well with\n",
    "the lower-most blob of our image of a 4, which looks kind of like a single\n",
    "horizontal line.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18378c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# It might be useful to start here with a list of all the kv pairs that activate\n",
    "# strongly for 2 on this particular image\n",
    "key_values_indices_that_code_strongly_for_2 = []\n",
    "\n",
    "for kv_pair_idx in top_key_value_pairs_for_img_of_four[:600]:\n",
    "  _, values = calculate_kv_activation_for_specific_kv(models[MODEL_IDX_WE_ARE_USING], image_of_four_in_test_set, kv_pair_idx)\n",
    "  if values[2] > 0.5:\n",
    "    key_values_indices_that_code_strongly_for_2.append(kv_pair_idx)\n",
    "\n",
    "print(f\"{key_values_indices_that_code_strongly_for_2=}\")\n",
    "\n",
    "# Unsurprisingly just using these values will make the model highly think that\n",
    "# the image is a 2.\n",
    "result_of_just_using_kv_indices_coding_strongly_for_2 = calculate_output_only_with_certain_kv_indices(\n",
    "  models[MODEL_IDX_WE_ARE_USING], \n",
    "  image_of_four_in_test_set,\n",
    "  key_values_indices_that_code_strongly_for_2,\n",
    ")\n",
    "print(f\"{result_of_just_using_kv_indices_coding_strongly_for_2=}\")\n",
    "\n",
    "# TODO: Scratch space for any code you want to write to come up with an explanation.\n",
    "# raise NotImplementedError()\n",
    "\n",
    "#BEGIN SOLUTION\n",
    "for kv_idx in key_values_indices_that_code_strongly_for_2[:10]:\n",
    "  visualize_ith_key_value_on_image(\n",
    "    models[MODEL_IDX_WE_ARE_USING], \n",
    "    kv_idx, \n",
    "    image_of_four_in_test_set.squeeze(),\n",
    "  )\n",
    "#END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e67d6e",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Our final main objective, before our bonus exercises, is to use the knowledge\n",
    "that we've gained here to selectively knock out the model's ability to recognize\n",
    "a single digit, without needing to resort to gradient descent to retrain the\n",
    "model.\n",
    "\n",
    "If we can do that, we've demonstrated a certain level of surgical insight into\n",
    "the model that goes beyond the standard \"black box\" thinking.\n",
    "\n",
    "Make sure you understand what's going on with `knock_out_ith_key` and\n",
    "`find_values_for_digit_over_threshold`.\n",
    "\n",
    "We'll be using that to knock out the model's ability to recognize 0s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4efbf32",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "import copy\n",
    "\n",
    "def delete_by_index(x: torch.Tensor, indices, dim: int = 0):\n",
    "    \"\"\"\n",
    "    Return a new tensor with the specified indices removed along `dim`.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    x (torch.Tensor): input tensor\n",
    "    indices (Sequence[int] | torch.Tensor): positions to delete\n",
    "    dim (int): dimension along which to delete (default 0)\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> t = torch.tensor([[10, 11],\n",
    "    ...                   [20, 21],\n",
    "    ...                   [30, 31],\n",
    "    ...                   [40, 41]])\n",
    "    >>> delete_by_index(t, [1, 3])\n",
    "    tensor([[10, 11],\n",
    "            [30, 31]])\n",
    "    \"\"\"\n",
    "    # Ensure we have a 1-D LongTensor of unique, sorted indices on the same device\n",
    "    idx = torch.as_tensor(indices, dtype=torch.long, device=x.device).unique().sort().values\n",
    "\n",
    "    # Build a boolean mask that is False at the indices we want to drop\n",
    "    mask_shape = [1] * x.dim()\n",
    "    mask_shape[dim] = x.size(dim)\n",
    "    mask = torch.ones(mask_shape, dtype=torch.bool, device=x.device).squeeze()\n",
    "    mask[idx] = False\n",
    "\n",
    "    return x[mask] if dim == 0 else x.transpose(0, dim)[mask].transpose(0, dim)\n",
    "\n",
    "#removes a certain key from the model\n",
    "def knock_out_ith_key(model: SimpleNN, key_value_idx: torch.Tensor) -> SimpleNN:\n",
    "  with torch.no_grad():\n",
    "    new_model = copy.deepcopy(model)\n",
    "    new_model.fc1 = torch.nn.Linear(model.fc1.in_features, model.fc1.out_features - key_value_idx.shape[0])\n",
    "    new_model.fc2 = torch.nn.Linear(model.fc2.in_features - key_value_idx.shape[0], model.fc2.out_features)\n",
    "    new_model.fc1.weight = torch.nn.Parameter(delete_by_index(model.fc1.weight, key_value_idx))\n",
    "    new_model.fc1.bias = torch.nn.Parameter(delete_by_index(model.fc1.bias, key_value_idx))\n",
    "    new_model.fc2.weight = torch.nn.Parameter(delete_by_index(model.fc2.weight, key_value_idx, dim=1))\n",
    "    return new_model\n",
    "\n",
    "def find_values_for_digit_over_threshold(model, digit, threshold=0.3):\n",
    "  return torch.tensor([idx for idx in range(model.fc2.weight.shape[1]) if model.fc2.weight[digit, idx] > threshold])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0b4e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Find all those key-value pairs which activate a lot for zero\n",
    "all_values_that_activate_significantly_for_zero = find_values_for_digit_over_threshold(models[MODEL_IDX_WE_ARE_USING], 0, threshold=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afae613f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's see if we can just selectively knock those out!\n",
    "model_with_0_knocked_out = knock_out_ith_key(models[MODEL_IDX_WE_ARE_USING], all_values_that_activate_significantly_for_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d57c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# And now we see that the model is basically entirely incapable of recognizing 0, but the rest of its capabilities are left intact!\n",
    "accuracy_by_digit(model_with_0_knocked_out.to(DEVICE), test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0d9fe6",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "*Exercise*: Can you do a similar thing for the digit 9, where we knock out the model's ability to recognize 9s?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc3cd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Scratchpad for exercise\n",
    "# raise NotImplementedError()\n",
    "\n",
    "#BEGIN SOLUTION\n",
    "# Find all those key-value pairs which activate a lot for zero\n",
    "all_values_that_activate_significantly_for_nine = find_values_for_digit_over_threshold(models[MODEL_IDX_WE_ARE_USING], 9, threshold=0.05)\n",
    "\n",
    "# Let's see if we can just selectively knock those out!\n",
    "model_with_9_knocked_out = knock_out_ith_key(models[MODEL_IDX_WE_ARE_USING], all_values_that_activate_significantly_for_nine)\n",
    "\n",
    "# And now we see that the model is basically entirely incapable of recognizing 0, but the rest of its capabilities are left intact!\n",
    "accuracy_by_digit(model_with_9_knocked_out.to(DEVICE), test_loader)\n",
    "#END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14aa0913",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "*Bonus Exercise*: Can you put everything together that we've learned so far in\n",
    "this workshop to craft an image that you think the neural net will get wrong?\n",
    "Even better can you predict what label the net will assign to the image\n",
    "instead?\n",
    "\n",
    "Feel free to do whatever analysis you want on the net, the only constraint is\n",
    "that you're not allowed to pass the image through the net itself (since that\n",
    "would be giving away the answer!)\n",
    "\n",
    "If you can do this without running the image through the net beforehand, this\n",
    "will demonstrate that you've pretty deeply understood the net!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f09e67e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# TODO: Scratchpad for exercise\n",
    "# raise NotImplementedError()\n",
    "\n",
    "#BEGIN SOLUTION\n",
    "# Here's our example, where the model gets a 6 very wrong because it expects 6s\n",
    "# to have big blobs in the middle of the image.\n",
    "tester = (torch.tensor(\n",
    "[[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "]).to(torch.float))\n",
    "\n",
    "visualize_image(tester)\n",
    "\n",
    "models[13](tester.unsqueeze(0))\n",
    "#END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dd5a98",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "*Bonus Exercise*: Visualize some of the key value pairs of the smallest model, both independently and on an image\n",
    "Are the visualizations explainable? Could you infer what some of the key-value pairs are doing?"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
