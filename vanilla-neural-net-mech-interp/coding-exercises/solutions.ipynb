{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b1f220",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341ddfa7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "# Some nice preliminary functions for testing.\n",
    "\n",
    "def assert_with_expect(expected, actual):\n",
    "    assert expected == actual, f\"Expected: {expected} Actual: {actual}\"\n",
    "\n",
    "\n",
    "def assert_list_of_floats_within_epsilon(\n",
    "    expected: list[float], \n",
    "    actual: list[float],\n",
    "    eps=0.0001,\n",
    "):\n",
    "    if len(expected) != len(actual):\n",
    "        raise AssertionError(f\"Expected: {expected} Actual: {actual}\")\n",
    "    is_within_eps = True\n",
    "    for e, a in zip(expected, actual):\n",
    "        is_within_eps = is_within_eps and abs(e - a) < eps\n",
    "    if not is_within_eps:\n",
    "        raise AssertionError(f\"Expected: {expected} Actual: {actual}\")\n",
    "\n",
    "\n",
    "def assert_tensors_within_epsilon(\n",
    "    expected: torch.Tensor,\n",
    "    actual: torch.Tensor,\n",
    "    eps=0.001,\n",
    "):\n",
    "    if expected.shape != actual.shape:\n",
    "        raise AssertionError(f\"Shapes of tensors do not match! Expected: {expected.shape} Acutal: {actual.shape}\")\n",
    "    differences_within_epsilon = abs(expected - actual) < eps\n",
    "    if not differences_within_epsilon.all():\n",
    "        raise AssertionError(f\"Values of tensors do not match! Expected: {expected} Actual: {actual}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03e53a2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "# None of this is code that you will need to write, but you should read this\n",
    "# over to understand the structure of what kind of nets we'll be training.\n",
    "#\n",
    "# Note that we only train with 10,000 images out of the 60,000 image dataset!\n",
    "# Originally this was because I was hoping to demonstrate some interesting\n",
    "# double descent phenomena, but unfortunately I ran out of time to do that :(.\n",
    "# Nonetheless, as we'll see, 10,000 images in the train set is actually enough\n",
    "# to get to a very well trained neural net!\n",
    "\n",
    "# hyper-params\n",
    "BATCH_SIZE = 512\n",
    "TRAIN_SET_SIZE = 10000\n",
    "HIDDEN_DIM = 256\n",
    "EPOCHS = 5000\n",
    "LR = 1e-3\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# If you set this to True then this will train all the models from scratch,\n",
    "# otherwise it will look for pre-saved weights and load those instead\n",
    "TRAIN_FROM_SCRATCH = False\n",
    "# When training, should we load the entire image set into GPU memory\n",
    "LOAD_EVERYTHING_INTO_GPU_MEMORY = True\n",
    "\n",
    "# simple with 3 layers\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(784, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x1 = self.relu(self.fc1(x))\n",
    "        x2 = self.fc2(x1)\n",
    "        return self.softmax(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af628d92",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "# This is useful for creating reproducible tests for whether you wrote the code\n",
    "# correctly!\n",
    "EXAMPLE_SIMPLE_NN_HIDDEN_DIM = 6\n",
    "example_simple_nn = SimpleNN(EXAMPLE_SIMPLE_NN_HIDDEN_DIM)\n",
    "\n",
    "# We'll hard-code all of these values so that they are reproducible for tests\n",
    "with torch.no_grad():\n",
    "  fc1_weight = einops.rearrange(torch.arange(784 * EXAMPLE_SIMPLE_NN_HIDDEN_DIM), \"(x y) -> x y\", x=EXAMPLE_SIMPLE_NN_HIDDEN_DIM, y=784)\n",
    "  fc1_bias = torch.arange(EXAMPLE_SIMPLE_NN_HIDDEN_DIM)\n",
    "  fc2_weight = einops.rearrange(torch.arange(10 * EXAMPLE_SIMPLE_NN_HIDDEN_DIM), \"(x y) -> x y\", x=10, y=EXAMPLE_SIMPLE_NN_HIDDEN_DIM)\n",
    "  fc2_bias = torch.arange(10)\n",
    "  example_simple_nn.fc1.weight = torch.nn.Parameter(fc1_weight.to(torch.float))\n",
    "  example_simple_nn.fc1.bias = torch.nn.Parameter(fc1_bias.to(torch.float))\n",
    "  example_simple_nn.fc2.weight = torch.nn.Parameter(fc2_weight.to(torch.float))\n",
    "  example_simple_nn.fc2.bias = torch.nn.Parameter(fc2_bias.to(torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50a8e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=10000)\n",
    "\n",
    "hidden_dims = [8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536, 131072]\n",
    "models = [SimpleNN(hidden_dim) for hidden_dim in hidden_dims]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326e49d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This is code that you can read if you'd like, but can also just run. It's\n",
    "# mainly useful if you wanted to train these models yourself.\n",
    "\n",
    "if LOAD_EVERYTHING_INTO_GPU_MEMORY:\n",
    "  # We'll load into memory to make this faster\n",
    "  train_loader_with_entire_dataset = torch.utils.data.DataLoader(train_dataset, batch_size=train_dataset.data.shape[0])\n",
    "  for batch_idx, (data, target) in enumerate(train_loader_with_entire_dataset):\n",
    "      data = data[:TRAIN_SET_SIZE].to(DEVICE)\n",
    "      target = torch.nn.functional.one_hot(target[:TRAIN_SET_SIZE], num_classes=10).to(torch.float)\n",
    "      target = target.to(DEVICE)\n",
    "\n",
    "  train_dataset = torch.utils.data.TensorDataset(data, target)\n",
    "  train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "  test_loader_with_entire_dataset = torch.utils.data.DataLoader(test_dataset, batch_size=test_dataset.data.shape[0])\n",
    "  for test_data, test_target in test_loader_with_entire_dataset:\n",
    "      test_data = test_data.to(DEVICE)\n",
    "      test_target = torch.nn.functional.one_hot(test_target, num_classes=10).to(torch.float)\n",
    "      test_target = test_target.to(DEVICE)\n",
    "\n",
    "\n",
    "  test_dataset = torch.utils.data.TensorDataset(test_data, test_target)\n",
    "  test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=10000)\n",
    "\n",
    "  # PyTorch DataLoader seems absurdly slow for MNIST dataset sizes\n",
    "  # It seems to be calling get_item one by one instead of doing batch operations\n",
    "  # Let's just do a custom list instead\n",
    "  def generate_simple_loader(dataset, batch_size):\n",
    "    permuted_indices = torch.randperm(dataset.tensors[0].shape[0])\n",
    "    permuted_data = dataset.tensors[0][permuted_indices]\n",
    "    permuted_target = dataset.tensors[1][permuted_indices]\n",
    "    simple_loader = []\n",
    "    for i in range(0, permuted_data.shape[0], batch_size):\n",
    "      simple_loader.append((permuted_data[i:i+batch_size], permuted_target[i:i+batch_size]))\n",
    "    return simple_loader\n",
    "\n",
    "  simple_train_loader = generate_simple_loader(train_dataset, BATCH_SIZE)\n",
    "  simple_test_loader = generate_simple_loader(test_dataset, 10000)\n",
    "\n",
    "  train_loader = simple_train_loader\n",
    "  test_loader = simple_test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df9fc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This is the actual training loop! Even though this is not code you will need\n",
    "# to write, you should definitely read this! It's good to understand exactly how\n",
    "# our model is being trained!\n",
    "#\n",
    "# You might notice that we're using MSELoss instead of cross-entropy loss. It\n",
    "# turns out that this is enough to get quite reasonable models and considerably\n",
    "# simplifies some of the presentataion to people who have only an introductory\n",
    "# understanding of neural nets.\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "for model in models:\n",
    "    print(f\"Processing hidden_dim {model.hidden_dim}\")\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "    # Training\n",
    "    train_loss = 0\n",
    "    train_accuracy = 0\n",
    "    train_samples = 0\n",
    "    if TRAIN_FROM_SCRATCH:\n",
    "      model = model.to(DEVICE)\n",
    "      for epoch in range(EPOCHS):\n",
    "          if LOAD_EVERYTHING_INTO_GPU_MEMORY:\n",
    "              # Re-shuffle the train loader\n",
    "              train_loader = generate_simple_loader(train_dataset, BATCH_SIZE)\n",
    "          for batch_idx, (data, target) in enumerate(train_loader):\n",
    "              optimizer.zero_grad()\n",
    "              output = model(data)\n",
    "              loss = criterion(output, target)\n",
    "              loss.backward()\n",
    "              optimizer.step()\n",
    "    else:\n",
    "      model.load_state_dict(torch.load(f\"mnist_model_hidden_layer_{model.hidden_dim}\"))\n",
    "      model = model.to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "      for data, target in train_loader:\n",
    "          output = model(data)\n",
    "          train_loss += criterion(output, target).item()\n",
    "          train_accuracy += (output.argmax(dim=1) == target.argmax(dim=1)).sum().item()\n",
    "          train_samples += data.shape[0]\n",
    "    train_losses.append(train_loss / len(train_loader))\n",
    "    train_accuracies.append(train_accuracy / train_samples)\n",
    "\n",
    "    # Testing\n",
    "    test_loss = 0\n",
    "    test_accuracy = 0\n",
    "    test_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for test_data, test_target in test_loader:\n",
    "            output = model(test_data)\n",
    "            loss = criterion(output, test_target)\n",
    "            test_loss += loss.item()\n",
    "            test_accuracy += (output.argmax(dim=1) == test_target.argmax(dim=1)).sum().item()\n",
    "            test_samples += test_data.shape[0]\n",
    "    test_losses.append(test_loss / len(test_loader))\n",
    "    test_accuracies.append(test_accuracy / test_samples)\n",
    "\n",
    "plt.plot(hidden_dims, train_losses, label='Train Loss')\n",
    "plt.plot(hidden_dims, test_losses, label='Test Loss')\n",
    "plt.xlabel('Hidden Dimension')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss vs Hidden Dim')\n",
    "plt.xscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(hidden_dims, train_accuracies, label='Train Accuracy')\n",
    "plt.plot(hidden_dims, test_accuracies, label='Test Accuracy')\n",
    "plt.xlabel('Hidden Dimension')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy vs Hidden Dim')\n",
    "plt.xscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682cddf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if TRAIN_FROM_SCRATCH:\n",
    "  for dim, model in zip(hidden_dims, models):\n",
    "    # Save on CPU because this makes it easier to load for more devices\n",
    "    model = model.to(\"cpu\")\n",
    "    torch.save(model.state_dict(), f\"mnist_model_hidden_layer_{dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bc4005",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "# Go ahead and run this just to make sure\n",
    "\n",
    "for model in models:\n",
    "  model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3fad6a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "# Fill this out! This should calculate the accuracy of the model for each digit.\n",
    "\n",
    "def accuracy_by_digit(model, loader):\n",
    "  # TODO: Implement this!\n",
    "  #raise NotImplementedError()\n",
    "  correct = [0] * 10\n",
    "  total = [0] * 10\n",
    "  with torch.no_grad():\n",
    "    for data, target_probs in loader:\n",
    "      output_probs = model(data)\n",
    "      output = output_probs.argmax(dim=1)\n",
    "      target = target_probs.argmax(dim=1)\n",
    "      for i in range(target.shape[0]):\n",
    "        total[target[i]] += 1\n",
    "        if output[i] == target[i]:\n",
    "          correct[target[i]] += 1\n",
    "  return [correct[i] / total[i] for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255a4d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test it out for our 131072 hidden units model\n",
    "accuracy_by_digit(models[14], test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26942f7d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "# Test it out for our 8 hidden units model\n",
    "accuracy_by_digit(models[0], test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bfa15a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "# Okay now we want to reframe our model using the key-value visualization\n",
    "# So it's time to implement the functions that will pull out the ith key and ith\n",
    "# value in our neural net.\n",
    "\n",
    "def pull_out_ith_key(model, i):\n",
    "  # TODO: Implement this (it should be just one line)\n",
    "  # raise NotImplementedError()\n",
    "  return model.fc1.weight[i]\n",
    "\n",
    "def pull_out_ith_value(model, i):\n",
    "  # TODO: Implement this (again should just be one line)\n",
    "  # raise NotImplementedError()\n",
    "  return model.fc2.weight[:, i]\n",
    "\n",
    "# Test code to make sure your code works!\n",
    "assert_tensors_within_epsilon(expected=torch.arange(1568, 2352), actual=pull_out_ith_key(example_simple_nn, 2))\n",
    "assert_tensors_within_epsilon(\n",
    "   expected=torch.tensor([ 2.,  8., 14., 20., 26., 32., 38., 44., 50., 56.]),\n",
    "   actual=pull_out_ith_value(example_simple_nn, 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971ee792",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "# This is all a lot of visualization code which you can either read or just run.\n",
    "\n",
    "#plots the image\n",
    "def visualize_image(image):\n",
    "  plt.imshow(image.detach().numpy(), cmap='viridis')\n",
    "  plt.axis('off')\n",
    "  plt.show()\n",
    "\n",
    "#plots a heatmap of a key\n",
    "def visualize_ith_key(model, i):\n",
    "  key = pull_out_ith_key(model, i).reshape(28, 28)\n",
    "  key_bias = model.fc1.bias[i]\n",
    "  plt.imshow(key.detach().numpy(), cmap='viridis')\n",
    "  plt.axis('off')\n",
    "  plt.title(f'Key {i} (bias: {key_bias})')\n",
    "  plt.show()\n",
    "\n",
    "#visualizes a value\n",
    "def visualize_ith_value(model, i):\n",
    "  value = pull_out_ith_value(model, i).unsqueeze(0)\n",
    "  plt.imshow(value.detach().numpy(), cmap='viridis')\n",
    "  for x in range(value.shape[1]):\n",
    "    plt.text(x, 0, f'{value[0, x].item():.3f}', ha='center', va='center', color='red', fontsize=6)\n",
    "  plt.axis('off')\n",
    "  plt.title(f'Value {i}')\n",
    "  plt.show()\n",
    "\n",
    "#visualizes the global value bias for each digit, or the baseline before any interactions\n",
    "def visualize_value_bias(model):\n",
    "  value = model.fc2.bias.unsqueeze(0)\n",
    "  plt.imshow(value.detach().numpy(), cmap='viridis')\n",
    "  for x in range(value.shape[1]):\n",
    "    plt.text(x, 0, f'{value[0, x].item():.3f}', ha='center', va='center', color='red', fontsize=6)\n",
    "  plt.axis('off')\n",
    "  plt.title(f'Global value bias')\n",
    "  plt.show()\n",
    "\n",
    "#combines the above 3 visualization functions\n",
    "def visualize_ith_key_value(model, i):\n",
    "  visualize_ith_key(model, i)\n",
    "  visualize_ith_value(model, i)\n",
    "  visualize_value_bias(model)\n",
    "\n",
    "#Shows most influential interaction areas between an image and key \n",
    "def visualize_element_wise_multi_of_key_image(model, i, image):\n",
    "  key = model.fc1.weight[i].reshape(28, 28)\n",
    "  element_wise_multi = key * image\n",
    "  plt.imshow(element_wise_multi.detach().numpy(), cmap='viridis')\n",
    "  plt.axis('off')\n",
    "  plt.title(f'Element-wise multiplication of key {i} with image')\n",
    "  plt.show()\n",
    "  print(f\"Dot-Product: {torch.sum(element_wise_multi)}\")\n",
    "\n",
    "#combines all of the above visualization functions\n",
    "def visualize_ith_key_value_on_image(model, i, image):\n",
    "  visualize_ith_key_value(model, i)\n",
    "  visualize_element_wise_multi_of_key_image(model, i, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6ffed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now that we have a way of pulling out keys and values, we can put that all\n",
    "# together to visualize a particular key-value pair!\n",
    "#\n",
    "# You might notice that this particular key (if you're using the pre-trained\n",
    "# model weights) looks visually kind of like a nine, and lo and behold, when you\n",
    "# go to the value vector that is getting written out, the highest activation is\n",
    "# a 9!\n",
    "\n",
    "visualize_ith_key_value(models[14].cpu(), 246)\n",
    "\n",
    "# Go ahead and play around with other key value pairs and see if you can make\n",
    "# sense of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb64565",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "# It's often useful to find which value vectors we have that tend to write\n",
    "# strongly for certain kinds of digits.\n",
    "#\n",
    "# Here is one very rough stab at the problem that just looks for any value\n",
    "# vector that has a value over a certain threshold for that digit. We'll quickly\n",
    "# show a slightly less rough stab in just a moment.\n",
    "\n",
    "def find_values_for_digit_over_threshold(model, digit, threshold=0.3):\n",
    "  return torch.tensor([idx for idx in range(model.fc2.weight.shape[1]) if model.fc2.weight[digit, idx] > threshold])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0c937a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This finds those values which have an entry of over 0.4 in the 0 digit place.\n",
    "\n",
    "find_values_for_digit_over_threshold(models[14], 0, threshold=0.4)\n",
    "\n",
    "# Feel free to feed this into visualize_ith_key_value to see what that key_value pair looks like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d453fafe",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "# Let's see a little bit more of how this key-value reframing of a vanilla\n",
    "# neural net can help us understand things better.\n",
    "#\n",
    "# For example, we might hypothesize that the key which corresponds to a value vector\n",
    "# that has a large positive value at 0 and small magnitude values for all other digits\n",
    "# should look like a circle.\n",
    "#\n",
    "# Note that this is not obviously true! It might be the case that a model pieces\n",
    "# together a zero exclusively by piecing together different arcs of a circle\n",
    "# with no key actually being a full circle.\n",
    "#\n",
    "# But we can go ahead and test that right now. First we'll need to build a\n",
    "# function that can find those key-value pairs which have values concentrated\n",
    "# mostly on one digit and not as much on the others.\n",
    "#\n",
    "# This can be a bit finicky and hard to specify, so we've provided a\n",
    "# rough-and-tumble version for you to use right here.\n",
    "\n",
    "def find_values_for_sole_digit(model, digit, digit_threshold=0.16, other_digits_threshold=0.07):\n",
    "  result = []\n",
    "  for idx in range(model.fc2.weight.shape[1]):\n",
    "    other_digits = torch.abs(model.fc2.weight[:, idx])\n",
    "    other_digits[digit] = 0\n",
    "    max_of_other_digits = torch.max(other_digits)\n",
    "    if max_of_other_digits.item() > other_digits_threshold:\n",
    "      continue\n",
    "    elif model.fc2.weight[digit, idx] > digit_threshold:\n",
    "      result.append(idx)\n",
    "  return torch.tensor(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcdfc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# So this for example finds those key-value pairs which tend to write very\n",
    "# strongly to the digit 2 happening, but very little for anything else.\n",
    "find_values_for_sole_digit(models[14], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09308f04",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "# Let's now find those key-value pairs which tend to write strongly to the digit\n",
    "# 0, but very little for everything else, and just analyze the first three of\n",
    "# those key-value pairs. This will let us validate our hypothesis of whether we\n",
    "# have keys that are looking for circles, or just fragmentary arcs of circles.\n",
    "\n",
    "digit_to_analyze = 0\n",
    "\n",
    "indices_that_fire_mainly_on_select_digit = find_values_for_sole_digit(models[14], digit_to_analyze)\n",
    "for idx in indices_that_fire_mainly_on_select_digit[:3]:\n",
    "  visualize_ith_key_value(models[14].to(\"cpu\"), idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16db9b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Look at the results and what they tell you. Talk with your partner or\n",
    "# group about what you're seeing. Once you've done that, delete this\n",
    "# NotImplementedError and move on.\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e624cf",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "# This function will give us the internal outputs of all the keys and values for\n",
    "# a given image. In other words this will return the dot product of each key\n",
    "# with the image (combined with the bias per key) and will also return the\n",
    "# scaled value vector.\n",
    "#\n",
    "# If this is confusing to you, it may be helpful to go back to the slides and\n",
    "# look a little bit more at the break-down of how exactly we calculate a neural\n",
    "# net's output using the key-value paradigm.\n",
    "\n",
    "def compute_kv_outputs_for_image(model, input_image):\n",
    "  flattened_img = model.flatten(input_image)\n",
    "  output_after_keys = model.fc1(flattened_img)\n",
    "  output_after_relu = model.relu(output_after_keys)\n",
    "  # We ultimately want to multiple all the components of each value vector by\n",
    "  # the same value, so we need to do a repeat first and then we can do a\n",
    "  # standard element-wise tensor multiplication\n",
    "  #\n",
    "  # But this is just the same as broadcasting, so we just use that instead\n",
    "  output_after_values = model.fc2.weight * output_after_relu\n",
    "  return output_after_keys, output_after_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195587e4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "def top_indices_by_tail_sum(tensor: torch.Tensor, threshold: float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Given a 1D tensor and a threshold, returns the indices of the largest values\n",
    "    such that the sum of all smaller values (i.e. the “tail” after that point)\n",
    "    is <= threshold.\n",
    "    \"\"\"\n",
    "    assert tensor.dim() == 1, \"Only works on 1D tensors\"\n",
    "    # Sort descending\n",
    "    sorted_vals, sorted_idx = tensor.sort(descending=True)\n",
    "    # Compute cumulative sum of the sorted values\n",
    "    cumsum = sorted_vals.cumsum(dim=0)\n",
    "    total = cumsum[-1]\n",
    "    # tail_sums[i] = sum(sorted_vals[i+1:])\n",
    "    tail_sums = total - cumsum\n",
    "    # find the first position where tail_sums <= threshold\n",
    "    mask = tail_sums <= threshold\n",
    "    if not mask.any():\n",
    "        # no cutoff—tail never drops below threshold, so return empty\n",
    "        return torch.empty(0, dtype=torch.long)\n",
    "    cutoff = mask.nonzero(as_tuple=False)[0].item()\n",
    "    # keep everything up to and including cutoff\n",
    "    return sorted_idx[:cutoff + 1]\n",
    "\n",
    "# Example\n",
    "x = torch.tensor([1, 4, 2, 3, 1], dtype=torch.float)\n",
    "indices = top_indices_by_tail_sum(x, threshold=4)\n",
    "print(f\"{indices=}\")  # tensor([1, 3])\n",
    "\n",
    "#returns the most influential key-value pairs for an image\n",
    "def list_top_kv_pair_idxs(model, input_image, excess_abs_weight=500):\n",
    "  _, output_after_values = compute_kv_outputs_for_image(model, input_image)\n",
    "  abs_values = einops.einsum(torch.abs(output_after_values), \"digits num_of_values -> num_of_values\")\n",
    "  indices = top_indices_by_tail_sum(abs_values, excess_abs_weight)\n",
    "  return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055b68db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's prove to ourselves that the key-value paradigm of calculating things is equal to the normal layer-by-layer interpretation\n",
    "def sanity_check_kv_outputs(model, input_image):\n",
    "  _, output_after_values = compute_kv_outputs_for_image(model, input_image)\n",
    "  output_plus_bias = einops.einsum(output_after_values, \"digits num_of_values -> digits\") + model.fc2.bias\n",
    "  print(f\"{output_plus_bias.softmax(dim=-1)=}\")\n",
    "  print(f\"{model(input_image)=}\")\n",
    "\n",
    "# You should see that the two print statements print the same values\n",
    "sanity_check_kv_outputs(models[14], train_dataset[0][0].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd674da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This will list the key-value pairs that write the value vectors with the largest magnitude.\n",
    "list_top_kv_pair_idxs(models[14], train_dataset[0][0].cpu(), 7000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ffd88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "visualize_image(train_dataset[0][0].cpu().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e87f36",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "visualize_ith_key_value_on_image(models[14], 14219, train_dataset[0][0].cpu().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfc3dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#finds the most variable key-value pairs\n",
    "def sort_by_value_variance(model, input_image):\n",
    "  _, output_after_values = compute_kv_outputs_for_image(model, input_image)\n",
    "  print(f\"{torch.var(output_after_values, dim=-1, keepdim=True).shape=}\")\n",
    "  variances = torch.var(output_after_values, dim=0, keepdim=True)\n",
    "  var_values, var_indices = torch.sort(variances, dim=-1, descending=True)\n",
    "  print(f\"{var_indices.shape=}\")\n",
    "  return var_indices\n",
    "\n",
    "top_5_kv_pairs_by_value_variance = sort_by_value_variance(models[14], train_dataset[0][0].cpu())[:, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153d94dd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "visualize_ith_key_value_on_image(models[14], 22650, train_dataset[0][0].cpu().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0946509a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#finds key-value pairs that react almost only to one digit\n",
    "def find_values_with_mostly_zeroes(model):\n",
    "  values = model.fc2.weight\n",
    "  num_of_elems_close_to_0 = torch.abs(values) < 0.05\n",
    "  print(f\"{values.shape=}\")\n",
    "  print(f\"{num_of_elems_close_to_0.shape=}\")\n",
    "  nine_elems_close_to_0 = torch.sum(num_of_elems_close_to_0, dim=0) == 9\n",
    "  indices_with_one_non_zero_elem = torch.nonzero(nine_elems_close_to_0).squeeze()\n",
    "  large_total_sums = torch.nonzero(torch.sum(values, dim=0) > 0.18).squeeze()\n",
    "  print(f\"{indices_with_one_non_zero_elem.shape=}\")\n",
    "  large_total_sum_and_nine_elems_close_to_0 = indices_with_one_non_zero_elem[torch.isin(indices_with_one_non_zero_elem, large_total_sums)]\n",
    "  print(f\"{large_total_sum_and_nine_elems_close_to_0=}\")\n",
    "\n",
    "find_values_with_mostly_zeroes(models[14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7257b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "visualize_ith_key_value(models[14].cpu(), 905)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06f887d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "visualize_image(train_dataset[5][0].cpu().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487ab38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "list_top_kv_pair_idxs(models[14], train_dataset[5][0].cpu(), 5500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519a3dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "visualize_ith_key_value_on_image(models[14].cpu(), 905, train_dataset[5][0].cpu().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ef20b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "visualize_image(train_dataset[3][0].cpu().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e128a033",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "list_top_kv_pair_idxs(models[0].cpu(), train_dataset[3][0].cpu(), 10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1d8bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models[0].cpu()(train_dataset[3][0].cpu()).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1ae776",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "visualize_ith_key_value_on_image(models[0].cpu(), 7, train_dataset[3][0].cpu().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4aa12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This finds the image that activates mostly strongly for a given key.\n",
    "\n",
    "def sort_highest_activating_image_for_key(model, key_value_idx, input_images):\n",
    "  key = model.fc1.weight[key_value_idx, :]\n",
    "  print(f\"{input_images.shape=}\")\n",
    "  flattened_images = model.flatten(input_images)\n",
    "  dot_products = einops.einsum(key, flattened_images, \"key_dim, batch key_dim -> batch\")\n",
    "  _, indices_by_dot_product = torch.sort(dot_products, descending=True)\n",
    "  return indices_by_dot_product\n",
    "\n",
    "train_images = torch.stack([img for img, _ in train_dataset])\n",
    "\n",
    "result = sort_highest_activating_image_for_key(models[14].cpu(), 905, train_images.cpu())\n",
    "\n",
    "print(f\"{result=}\")\n",
    "\n",
    "visualize_image(train_images[result][5].cpu().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b3a3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "visualize_ith_key_value_on_image(models[14].cpu(), 905, train_dataset[2019][0].cpu().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51721ba8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "import copy\n",
    "\n",
    "def delete_by_index(x: torch.Tensor, indices, dim: int = 0):\n",
    "    \"\"\"\n",
    "    Return a new tensor with the specified indices removed along `dim`.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    x (torch.Tensor): input tensor\n",
    "    indices (Sequence[int] | torch.Tensor): positions to delete\n",
    "    dim (int): dimension along which to delete (default 0)\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> t = torch.tensor([[10, 11],\n",
    "    ...                   [20, 21],\n",
    "    ...                   [30, 31],\n",
    "    ...                   [40, 41]])\n",
    "    >>> delete_by_index(t, [1, 3])\n",
    "    tensor([[10, 11],\n",
    "            [30, 31]])\n",
    "    \"\"\"\n",
    "    # Ensure we have a 1-D LongTensor of unique, sorted indices on the same device\n",
    "    idx = torch.as_tensor(indices, dtype=torch.long, device=x.device).unique().sort().values\n",
    "\n",
    "    # Build a boolean mask that is False at the indices we want to drop\n",
    "    mask_shape = [1] * x.dim()\n",
    "    mask_shape[dim] = x.size(dim)\n",
    "    mask = torch.ones(mask_shape, dtype=torch.bool, device=x.device).squeeze()\n",
    "    mask[idx] = False\n",
    "\n",
    "    return x[mask] if dim == 0 else x.transpose(0, dim)[mask].transpose(0, dim)\n",
    "\n",
    "#removes a certain key from the model\n",
    "def knock_out_ith_key(model: SimpleNN, key_value_idx: torch.Tensor) -> SimpleNN:\n",
    "  with torch.no_grad():\n",
    "    new_model = copy.deepcopy(model)\n",
    "    new_model.fc1 = torch.nn.Linear(model.fc1.in_features, model.fc1.out_features - key_value_idx.shape[0])\n",
    "    new_model.fc2 = torch.nn.Linear(model.fc2.in_features - key_value_idx.shape[0], model.fc2.out_features)\n",
    "    new_model.fc1.weight = torch.nn.Parameter(delete_by_index(model.fc1.weight, key_value_idx))\n",
    "    new_model.fc1.bias = torch.nn.Parameter(delete_by_index(model.fc1.bias, key_value_idx))\n",
    "    new_model.fc2.weight = torch.nn.Parameter(delete_by_index(model.fc2.weight, key_value_idx, dim=1))\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b75218",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Find all those key-value pairs which activate a lot for zero\n",
    "all_values_that_activate_significantly_for_zero = find_values_for_digit_over_threshold(models[14], 0, threshold=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5884c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's see if we can just selectively knock those out!\n",
    "model_with_0_knocked_out = knock_out_ith_key(models[14], all_values_that_activate_significantly_for_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89f738f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# And now we see that the model is basically entirely incapable of recognizing 0, but the rest of its capabilities are left intact!\n",
    "accuracy_by_digit(model_with_0_knocked_out.to(DEVICE), test_loader)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
