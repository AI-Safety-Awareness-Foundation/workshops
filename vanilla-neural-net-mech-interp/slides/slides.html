<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title>slides</title>
  <style type="text/css">
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" type="text/css" media="screen, projection, print"
    href="https://www.w3.org/Talks/Tools/Slidy2/styles/slidy.css" />
  <script src="https://www.w3.org/Talks/Tools/Slidy2/scripts/slidy.js"
    charset="utf-8" type="text/javascript"></script>
</head>
<body>
<div id="mechanistic-interpretability-on-vanilla-neural-nets-workshop"
class="slide section level1">
<h1>Mechanistic Interpretability on Vanilla Neural Nets Workshop</h1>
<p>Understanding How a Vanilla Neural Net Does Image Detection from the
Inside Out</p>
</div>
<div id="overall-structure-today" class="slide section level1">
<h1>Overall Structure Today</h1>
<ul class="incremental">
<li>We’ll assume:
<ul class="incremental">
<li>You know</li>
</ul></li>
</ul>
</div>
<div id="the-ai-safety-challenge" class="slide section level1">
<h1>The AI Safety Challenge</h1>
<ul class="incremental">
<li>We are building increasingly powerful AI systems</li>
<li>Current trajectory points toward AGI (Artificial General
Intelligence) and potentially ASI (Artificial Superintelligence)</li>
<li><strong>Critical problem</strong>: We don’t understand how these
systems work internally</li>
<li>We deploy systems we cannot:
<ul class="incremental">
<li>Fully predict</li>
<li>Reliably control</li>
<li>Comprehensively debug</li>
</ul></li>
</ul>
</div>
<div id="why-this-matters" class="slide section level1">
<h1>Why This Matters</h1>
<ul class="incremental">
<li>AI systems are already making consequential decisions:
<ul class="incremental">
<li>Medical diagnoses</li>
<li>Loan approvals</li>
<li>Criminal justice recommendations</li>
<li>Autonomous vehicle navigation</li>
</ul></li>
<li>As capabilities increase, so do potential risks:
<ul class="incremental">
<li>Misaligned objectives</li>
<li>Unintended optimization targets</li>
<li>Deceptive behavior</li>
<li>Catastrophic failures at scale</li>
</ul></li>
</ul>
</div>
<div id="we-might-not-have-a-lot-of-time" class="slide section level1">
<h1>We Might Not Have a Lot of Time</h1>
<ul class="incremental">
<li>There’s a chance we might get AGI quite soon
<ul class="incremental">
<li>What</li>
</ul></li>
</ul>
</div>
<div id="the-black-box-problem" class="slide section level1">
<h1>The Black Box Problem</h1>
<div style="text-align: center; font-size: 1.5em; margin: 40px;">
<p>Input → [???] → Output</p>
</div>
<ul class="incremental">
<li>Modern AI systems contain billions or trillions of parameters</li>
<li>We know the architecture and weights</li>
<li>But we don’t know <strong>what the model has learned</strong> or
<strong>how it makes decisions</strong></li>
<li>This opacity becomes dangerous with sufficiently powerful
systems</li>
</ul>
</div>
<div id="mechanistic-interpretability-one-bet"
class="slide section level1">
<h1>Mechanistic Interpretability: One Bet</h1>
<ul class="incremental">
<li><strong>Goal</strong>: Reverse-engineer neural networks to
understand their internal mechanisms</li>
<li><strong>Promise</strong>: If we can understand how models work, we
might:
<ul class="incremental">
<li>Detect deception</li>
<li>Identify dangerous capabilities</li>
<li>Build safer systems</li>
<li>Verify alignment</li>
</ul></li>
<li><strong>Reality</strong>: Current progress is promising but severely
limited</li>
</ul>
</div>
<div id="why-study-mechanistic-interpretability-anyway"
class="slide section level1">
<h1>Why Study Mechanistic Interpretability Anyway?</h1>
<ol class="incremental" style="list-style-type: decimal">
<li><strong>Building foundations</strong>: Today’s toy models teach us
techniques for tomorrow’s challenges</li>
<li><strong>Developing intuitions</strong>: Understanding simple systems
helps us reason about complex ones</li>
<li><strong>Creating tools</strong>: Methods developed now may scale
with future innovations</li>
<li><strong>Inspiring solutions</strong>: Deep understanding often
precedes breakthrough insights</li>
<li><strong>It’s plain beautiful</strong>: Mechanistic interpretability
can be fun!</li>
</ol>
</div>
<div id="current-state-of-the-field" class="slide section level1">
<h1>Current State of the Field</h1>
<h2 id="what-we-can-do">What we can do:</h2>
<ul class="incremental">
<li>Identify some circuits in small models</li>
<li>Understand basic features in vision models</li>
<li>Find some interpretable directions in language models</li>
</ul>
<h2 id="what-we-cannot-do">What we <strong>cannot</strong> do:</h2>
<ul class="incremental">
<li>Fully interpret even small models</li>
<li>Reliably detect deception</li>
<li>Predict emergent capabilities</li>
<li>Scale our techniques to frontier models</li>
</ul>
</div>
<div id="reality-is-not-great" class="slide section level1">
<h1>Reality is Not Great</h1>
<ul class="incremental">
<li><strong>No current AI safety approach is on track to solve AGI/ASI
alignment</strong>
<ul class="incremental">
<li>There are reasons</li>
<li>Including mechanistic interpretability</li>
</ul></li>
<li>The gap between our understanding and our capabilities is
<strong>growing</strong>, not shrinking</li>
<li>This is one of the most urgent technical challenges in the world at
the moment
<ul class="incremental">
<li>Which is why we need more brilliant people working on this!</li>
</ul></li>
<li><strong>We have many bets, but we don’t know if one or any will pan
out.</strong>
<ul class="incremental">
<li>This is a bad situation!</li>
<li>But not hopeless: this is where AI capabilities was before LLMs
rolled around</li>
</ul></li>
</ul>
</div>
<div id="todays-workshop" class="slide section level1">
<h1>Today’s Workshop</h1>
<ul class="incremental">
<li>We’ll study a <strong>vanilla neural network</strong> - vastly
simpler than modern systems</li>
<li>Even this simple model exhibits surprising complexity</li>
<li>You’ll learn fundamental mechanistic interpretability
techniques</li>
<li>The general contour of the work applies to larger models nad more
complex architectures</li>
</ul>
</div>
<div id="mechanistic-interpretability-vanilla-neural-nets"
class="slide section level1">
<h1>Mechanistic Interpretability (Vanilla Neural Nets)</h1>
<ul class="incremental">
<li>Assume you know what a neural net is</li>
<li>What is mechanistic interpretability?
<ul class="incremental">
<li>Interpretability: trying to find a way to understand something</li>
<li>Mechanistic: finding the specific, concrete mechanisms used
<ul class="incremental">
<li>Ideally you could drill down to the per-neuron level</li>
</ul></li>
</ul></li>
<li>We want to delve into the black box</li>
</ul>
</div>
<div id="common-themes-of-mechanistic-interpretability"
class="slide section level1">
<h1>Common themes of mechanistic interpretability</h1>
<ul class="incremental">
<li>Finding mathematically equivalent or near-equivalent restructurings
of an architecture</li>
<li>Being able to identify specific, concrete parts of a model that are
responsible for</li>
</ul>
</div>
<div id="our-object-of-study" class="slide section level1">
<h1>Our object of study</h1>
<ul class="incremental">
<li>Vanilla neural net (single hidden layer)</li>
<li>Simple image recognition</li>
<li>Much simpler than modern LLMs</li>
<li>But still complex enough to demonstrate the fundamental themes of
mechanistic interpretability</li>
</ul>
</div>
<div id="what-youll-be-able-to-do-at-the-end-of-the-day"
class="slide section level1">
<h1>What you’ll be able to do at the end of the day</h1>
<ul class="incremental">
<li>Get a new look at the traditional vanilla neural net
architecture</li>
<li>Thoroughly and mechanistically explain why the NN gets certain
digits wrong (and right)</li>
</ul>
</div>
<div id="standard-way-of-thinking-about-a-single-hidden-layer-nn"
class="slide section level1">
<h1>Standard way of thinking about a single hidden layer NN</h1>
<ul class="incremental">
<li>There are two (three if you count inputs as neurons) layers of
neurons</li>
<li>We pass iteratively from one layer to the next</li>
<li>Therefore to understand why</li>
</ul>
</div>
<div id="diagram-of-standard-neural-net" class="slide section level1">
<h1>Diagram of standard neural net</h1>
<p><img src="./neural-net.svg"/></p>
</div>
<div id="standard-layer-by-layer-decomposition"
class="slide section level1">
<h1>Standard layer by layer decomposition</h1>
<p><img src="./neural-net-layer-decomposition.svg" width="500px"/></p>
</div>
<div id="alternative-decomposition-key-value"
class="slide section level1">
<h1>Alternative decomposition: key-value</h1>
<p><img src="./neural-net-key-value-decomposition.svg" width="700px"/></p>
</div>
<div id="calculate-as-key-value-decomposition"
class="slide section level1">
<h1>Calculate as “key-value” decomposition</h1>
<p><img src="./neural-net-key-value-calculation.svg" width="500px"/></p>
</div>
</body>
</html>
